{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f9233a",
   "metadata": {},
   "source": [
    "# üöÄ Confidence Interval Metrics - Interactive Demo\n",
    "\n",
    "This notebook demonstrates how to use the **MetricEvaluator** unified interface for calculating confidence intervals on both regression and classification metrics.\n",
    "\n",
    "## üìã What You'll Learn:\n",
    "- How to use the unified `MetricEvaluator` class\n",
    "- Calculate confidence intervals for regression metrics (MAE, MSE, RMSE, R¬≤)  \n",
    "- Calculate confidence intervals for classification metrics (accuracy, precision, recall)\n",
    "- Different confidence interval methods (bootstrap, jackknife, proportion-based)\n",
    "- Interactive examples with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bbb40",
   "metadata": {},
   "source": [
    "## üì¶ Install and Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27724985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b391f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confidence interval package\n",
    "import sys\n",
    "sys.path.append('/home/zokirov_diyorbek/Documents/confidence_interval_jacob/confidenceinterval')\n",
    "\n",
    "from confidenceinterval import MetricEvaluator\n",
    "\n",
    "print(\"‚úÖ MetricEvaluator imported successfully!\")\n",
    "print(f\"üìä Available for regression: {MetricEvaluator().get_available_metrics('regression')}\")\n",
    "print(f\"üéØ Available for classification: {MetricEvaluator().get_available_metrics('classification')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b177a",
   "metadata": {},
   "source": [
    "## üìà Regression Metrics with Confidence Intervals\n",
    "\n",
    "Let's start with regression metrics. We'll generate some sample data, train a model, and then calculate confidence intervals for various regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample regression data\n",
    "print(\"üîÑ Generating regression dataset...\")\n",
    "X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a regression model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"üìä Dataset: {len(X_test)} test samples\")\n",
    "print(f\"üéØ R¬≤ Score: {model.score(X_test, y_test):.4f}\")\n",
    "print(f\"üìâ MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions vs Actual')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for regression metrics\n",
    "evaluator = MetricEvaluator()\n",
    "\n",
    "# Test different regression metrics\n",
    "regression_metrics = ['mae', 'mse', 'rmse', 'r2']\n",
    "methods = ['bootstrap_bca', 'jackknife']\n",
    "\n",
    "print(\"üîç REGRESSION METRICS WITH CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for metric in regression_metrics:\n",
    "    print(f\"\\nüìä {metric.upper()} Results:\")\n",
    "    for method in methods:\n",
    "        try:\n",
    "            score, ci = evaluator.evaluate(\n",
    "                y_true=y_test.tolist(),\n",
    "                y_pred=y_pred.tolist(),\n",
    "                task='regression',\n",
    "                metric=metric,\n",
    "                method=method,\n",
    "                confidence_level=0.95\n",
    "            )\n",
    "            print(f\"  {method:15s}: {score:.6f}, CI: ({ci[0]:.6f}, {ci[1]:.6f})\")\n",
    "            results_data.append({\n",
    "                'Metric': metric.upper(),\n",
    "                'Method': method,\n",
    "                'Score': score,\n",
    "                'CI_Lower': ci[0],\n",
    "                'CI_Upper': ci[1],\n",
    "                'CI_Width': ci[1] - ci[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  {method:15s}: ‚ùå Failed - {e}\")\n",
    "\n",
    "# Create a results DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(f\"\\nüìã Summary of {len(results_data)} successful calculations:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0449121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence intervals for regression metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Regression Metrics with 95% Confidence Intervals', fontsize=16)\n",
    "\n",
    "metrics = results_df['Metric'].unique()\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    metric_data = results_df[results_df['Metric'] == metric]\n",
    "    \n",
    "    x_positions = range(len(metric_data))\n",
    "    scores = metric_data['Score'].values\n",
    "    ci_lower = metric_data['CI_Lower'].values\n",
    "    ci_upper = metric_data['CI_Upper'].values\n",
    "    methods = metric_data['Method'].values\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    bars = ax.bar(x_positions, scores, color=colors, alpha=0.7, \n",
    "                  yerr=[scores - ci_lower, ci_upper - scores], \n",
    "                  capsize=10, error_kw={'linewidth': 2})\n",
    "    \n",
    "    ax.set_title(f'{metric} with Confidence Intervals')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(methods, rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, (score, method) in enumerate(zip(scores, methods)):\n",
    "        ax.text(j, score, f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Interpretation:\")\n",
    "print(\"- Error bars show 95% confidence intervals\")\n",
    "print(\"- Smaller intervals indicate more precise estimates\")\n",
    "print(\"- Bootstrap and Jackknife methods may give different interval widths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba044a9",
   "metadata": {},
   "source": [
    "## üéØ Classification Metrics with Confidence Intervals\n",
    "\n",
    "Now let's explore classification metrics. We'll generate binary classification data and calculate confidence intervals for common classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample classification data\n",
    "print(\"üîÑ Generating classification dataset...\")\n",
    "X_clf, y_clf = make_classification(n_samples=300, n_features=10, n_classes=2, \n",
    "                                   n_informative=7, n_redundant=1, random_state=42)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a classification model\n",
    "clf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_model.fit(X_train_clf, y_train_clf)\n",
    "y_pred_clf = clf_model.predict(X_test_clf)\n",
    "y_pred_proba = clf_model.predict_proba(X_test_clf)[:, 1]\n",
    "\n",
    "print(f\"üìä Dataset: {len(X_test_clf)} test samples\")\n",
    "print(f\"üéØ Accuracy: {accuracy_score(y_test_clf, y_pred_clf):.4f}\")\n",
    "print(f\"üìà Class distribution: {np.bincount(y_test_clf)}\")\n",
    "\n",
    "# Plot classification results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y_pred_proba[y_test_clf == 0], alpha=0.7, label='Class 0', bins=20)\n",
    "plt.hist(y_pred_proba[y_test_clf == 1], alpha=0.7, label='Class 1', bins=20)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Probability Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test_clf, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for classification metrics\n",
    "classification_metrics = ['accuracy', 'precision', 'recall']\n",
    "classification_methods = ['wilson', 'normal', 'agresti_coull']\n",
    "\n",
    "print(\"üîç CLASSIFICATION METRICS WITH CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clf_results_data = []\n",
    "\n",
    "for metric in classification_metrics:\n",
    "    print(f\"\\nüéØ {metric.upper()} Results:\")\n",
    "    for method in classification_methods:\n",
    "        try:\n",
    "            score, ci = evaluator.evaluate(\n",
    "                y_true=y_test_clf.tolist(),\n",
    "                y_pred=y_pred_clf.tolist(),\n",
    "                task='classification',\n",
    "                metric=metric,\n",
    "                method=method,\n",
    "                confidence_level=0.95\n",
    "            )\n",
    "            print(f\"  {method:15s}: {score:.6f}, CI: ({ci[0]:.6f}, {ci[1]:.6f})\")\n",
    "            clf_results_data.append({\n",
    "                'Metric': metric.upper(),\n",
    "                'Method': method,\n",
    "                'Score': score,\n",
    "                'CI_Lower': ci[0],\n",
    "                'CI_Upper': ci[1],\n",
    "                'CI_Width': ci[1] - ci[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  {method:15s}: ‚ùå Failed - {e}\")\n",
    "\n",
    "# Create a results DataFrame\n",
    "clf_results_df = pd.DataFrame(clf_results_data)\n",
    "print(f\"\\nüìã Summary of {len(clf_results_data)} successful calculations:\")\n",
    "print(clf_results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification confidence intervals\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Classification Metrics with 95% Confidence Intervals', fontsize=16)\n",
    "\n",
    "clf_metrics = clf_results_df['Metric'].unique()\n",
    "colors = ['lightgreen', 'lightblue', 'salmon']\n",
    "\n",
    "for i, metric in enumerate(clf_metrics):\n",
    "    ax = axes[i]\n",
    "    metric_data = clf_results_df[clf_results_df['Metric'] == metric]\n",
    "    \n",
    "    x_positions = range(len(metric_data))\n",
    "    scores = metric_data['Score'].values\n",
    "    ci_lower = metric_data['CI_Lower'].values\n",
    "    ci_upper = metric_data['CI_Upper'].values\n",
    "    methods = metric_data['Method'].values\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    bars = ax.bar(x_positions, scores, color=colors[i], alpha=0.7, \n",
    "                  yerr=[scores - ci_lower, ci_upper - scores], \n",
    "                  capsize=8, error_kw={'linewidth': 2})\n",
    "    \n",
    "    ax.set_title(f'{metric}')\n",
    "    ax.set_xlabel('CI Method')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(methods, rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, (score, method) in enumerate(zip(scores, methods)):\n",
    "        ax.text(j, score + 0.02, f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Different confidence interval methods for classification:\")\n",
    "print(\"- Wilson: Generally recommended, good coverage properties\")\n",
    "print(\"- Normal: Simple but may have poor coverage for extreme proportions\")  \n",
    "print(\"- Agresti-Coull: Conservative, good for small samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4299ef2",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Examples - Try Your Own Data!\n",
    "\n",
    "Let's create some simple examples you can modify with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Simple Regression Example - Modify these values!\n",
    "print(\"üìä SIMPLE REGRESSION EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Your data here - feel free to modify!\n",
    "y_true_reg = [1.0, 2.5, 3.2, 4.1, 5.0, 2.8, 3.9, 4.7, 1.8, 2.3]\n",
    "y_pred_reg = [1.1, 2.4, 3.0, 4.2, 4.9, 2.9, 3.8, 4.8, 1.9, 2.2]\n",
    "\n",
    "evaluator = MetricEvaluator()\n",
    "\n",
    "# Calculate different metrics\n",
    "metrics_simple = ['mae', 'mse', 'rmse', 'r2']\n",
    "\n",
    "print(\"Results with Bootstrap BCA:\")\n",
    "for metric in metrics_simple:\n",
    "    score, ci = evaluator.evaluate(y_true_reg, y_pred_reg, \n",
    "                                  task='regression', metric=metric, \n",
    "                                  method='bootstrap_bca')\n",
    "    print(f\"{metric.upper():4s}: {score:7.4f}, CI: ({ci[0]:7.4f}, {ci[1]:7.4f})\")\n",
    "\n",
    "# Visualize the simple regression data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(range(len(y_true_reg)), y_true_reg, label='True', alpha=0.7, s=50)\n",
    "plt.scatter(range(len(y_pred_reg)), y_pred_reg, label='Predicted', alpha=0.7, s=50)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_true_reg, y_pred_reg, alpha=0.7)\n",
    "plt.plot([min(y_true_reg), max(y_true_reg)], [min(y_true_reg), max(y_true_reg)], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Prediction Scatter Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Simple Classification Example - Modify these values!\n",
    "print(\"\\nüéØ SIMPLE CLASSIFICATION EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Your classification data here - feel free to modify!\n",
    "y_true_clf_simple = [0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
    "y_pred_clf_simple = [0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "print(f\"Sample size: {len(y_true_clf_simple)}\")\n",
    "print(f\"True positives: {sum(1 for t, p in zip(y_true_clf_simple, y_pred_clf_simple) if t == 1 and p == 1)}\")\n",
    "print(f\"True negatives: {sum(1 for t, p in zip(y_true_clf_simple, y_pred_clf_simple) if t == 0 and p == 0)}\")\n",
    "\n",
    "print(\"\\nResults with Wilson method:\")\n",
    "clf_metrics_simple = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "for metric in clf_metrics_simple:\n",
    "    score, ci = evaluator.evaluate(y_true_clf_simple, y_pred_clf_simple, \n",
    "                                  task='classification', metric=metric, \n",
    "                                  method='wilson')\n",
    "    print(f\"{metric.upper():9s}: {score:6.4f}, CI: ({ci[0]:6.4f}, {ci[1]:6.4f})\")\n",
    "\n",
    "# Create a simple confusion matrix visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true_clf_simple, y_pred_clf_simple)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Pred 0', 'Pred 1'], \n",
    "            yticklabels=['True 0', 'True 1'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall']\n",
    "metrics_values = []\n",
    "for metric in ['accuracy', 'precision', 'recall']:\n",
    "    score, _ = evaluator.evaluate(y_true_clf_simple, y_pred_clf_simple, \n",
    "                                 task='classification', metric=metric)\n",
    "    metrics_values.append(score)\n",
    "\n",
    "bars = plt.bar(metrics_names, metrics_values, color=['lightgreen', 'lightblue', 'salmon'], alpha=0.7)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Metrics')\n",
    "plt.ylim(0, 1)\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a4202",
   "metadata": {},
   "source": [
    "## üìö Quick Reference & Summary\n",
    "\n",
    "Here's everything you need to know to use the MetricEvaluator in your own projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7664a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Quick Reference Guide\n",
    "evaluator = MetricEvaluator()\n",
    "\n",
    "print(\"üöÄ METRICEVAL UATOR QUICK REFERENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìä Available Regression Metrics:\")\n",
    "reg_metrics = evaluator.get_available_metrics('regression')\n",
    "for i, metric in enumerate(reg_metrics, 1):\n",
    "    print(f\"  {i}. {metric}\")\n",
    "\n",
    "print(\"\\nüéØ Available Classification Metrics:\")\n",
    "clf_metrics = evaluator.get_available_metrics('classification')\n",
    "for i, metric in enumerate(clf_metrics, 1):\n",
    "    print(f\"  {i}. {metric}\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Available Methods for Regression:\")\n",
    "reg_methods = evaluator.get_available_methods('regression')\n",
    "for i, method in enumerate(reg_methods, 1):\n",
    "    print(f\"  {i}. {method}\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Available Methods for Classification:\")\n",
    "clf_methods = evaluator.get_available_methods('classification')\n",
    "for i, method in enumerate(clf_methods, 1):\n",
    "    print(f\"  {i}. {method}\")\n",
    "\n",
    "print(\"\\nüí° Basic Usage Pattern:\")\n",
    "print(\"\"\"\n",
    "from confidenceinterval import MetricEvaluator\n",
    "\n",
    "evaluator = MetricEvaluator()\n",
    "\n",
    "# For regression\n",
    "score, ci = evaluator.evaluate(\n",
    "    y_true=[1.0, 2.0, 3.0],\n",
    "    y_pred=[1.1, 2.1, 2.9], \n",
    "    task='regression',\n",
    "    metric='mae',\n",
    "    method='bootstrap_bca',\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\n",
    "# For classification  \n",
    "score, ci = evaluator.evaluate(\n",
    "    y_true=[0, 1, 1, 0],\n",
    "    y_pred=[0, 1, 0, 0],\n",
    "    task='classification', \n",
    "    metric='accuracy',\n",
    "    method='wilson',\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6ca20",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully learned how to use the **MetricEvaluator** for calculating confidence intervals on machine learning metrics!\n",
    "\n",
    "### üîë Key Takeaways:\n",
    "- **Unified Interface**: One class handles both regression and classification metrics\n",
    "- **Multiple Methods**: Bootstrap, Jackknife, and proportion-based confidence intervals  \n",
    "- **Easy to Use**: Simple `.evaluate()` method with clear parameters\n",
    "- **Comprehensive**: Supports all common evaluation metrics\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Try the examples with your own data\n",
    "2. Experiment with different confidence levels (e.g., 0.90, 0.99)\n",
    "3. Compare different confidence interval methods\n",
    "4. Use in your machine learning projects for robust evaluation\n",
    "\n",
    "### üìñ Remember:\n",
    "- Confidence intervals help quantify uncertainty in your metrics\n",
    "- Larger samples generally give tighter confidence intervals  \n",
    "- Different methods may be more appropriate for different scenarios\n",
    "- Always report confidence intervals alongside point estimates!\n",
    "\n",
    "**Happy analyzing! üìä‚ú®**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
