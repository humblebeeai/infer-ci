{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Model evaluation and confidence interval validation</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Python module/package</li> <li>Configuration</li> <li>Test</li> <li>Build</li> <li>Documentation</li> <li>Scripts</li> <li>Examples</li> <li>CI/CD</li> </ul>"},{"location":"blog/","title":"Your model is not as good as you think. Or maybe it is. Without uncertainty, you cannot tell.","text":"<p>Machine learning teams love clean numbers: Accuracy of 92%, RMSE of 3.4, F1-score of 0.88. These values look decisive, but they hide a critical truth: every metric is only an estimate. It depends on data sampling, class imbalance, noise, and evaluation protocol. Confidence intervals turn these fragile point estimates into something you can trust.</p> <p>In this post, we will explore why confidence intervals matter in modern ML and DL, then introduce Infer, a project built to make metric uncertainty first-class. We will walk through how Infer works and how it supports both classification and regression metrics using a clean, end-to-end evaluation flow.</p>"},{"location":"blog/#why-confidence-intervals-matter-in-ml-and-dl","title":"Why confidence intervals matter in ML and DL","text":"<p>A single metric value answers only one question: what happened on this test set?</p> <p>Confidence intervals answer a more important one: how reliable is this result?</p> <p>In real-world ML systems, uncertainty shows up everywhere:</p> <ul> <li>Small or imbalanced datasets inflate variance</li> <li>Cross-validation folds produce inconsistent scores</li> <li>Deployment data drifts from offline evaluation</li> <li>Two models with the same accuracy may behave very differently in production</li> </ul> <p>Confidence intervals quantify this uncertainty by estimating a range within which the true metric value is likely to lie. Instead of saying \u201caccuracy is 92%\u201d, you say \u201caccuracy is 92% \u00b1 1.5% at 95% confidence\u201d. This changes how decisions are made.</p> <p>Confidence intervals help you:</p> <ul> <li>Compare models statistically, not emotionally</li> <li>Avoid overconfidence on noisy or small datasets</li> <li>Detect when improvements are not statistically meaningful</li> <li>Communicate reliability to stakeholders and regulators</li> <li>Make safer decisions in high-risk domains like healthcare or finance</li> </ul> <p>Despite all this, CI computation for ML metrics is still fragmented, inconsistent, and often ignored.</p> <p>That gap is exactly why Infer exists.</p>"},{"location":"blog/#introducing-infer","title":"Introducing Infer","text":"<p>Infer is a Python-based evaluation framework designed to compute confidence intervals for machine learning metrics in a standardized, reliable, and extensible way.</p> <p>Infer focuses on a simple idea: metrics without uncertainty are incomplete. The project provides a unified pipeline that starts from exploratory data analysis and ends with interpretable outputs that include metrics, confidence intervals, and visualizations.</p> <p>At its core, Infer supports:</p> <ul> <li>Classification metrics like Accuracy, Precision, Recall, F1-score, ROC-AUC</li> <li>Regression metrics like MAE, RMSE, MAPE</li> <li>Multiple CI estimation methods including analytical and bootstrap-based approaches</li> <li>Automatic method selection based on metric and data characteristics</li> <li>Visual outputs that make uncertainty visible, not abstract</li> </ul> <p>This design is rooted in the observation that CI computation has not been standardized across ML tooling, even though robust statistical methods are well-known and widely studied.</p>"},{"location":"blog/#the-infer-evaluation-flow","title":"The Infer evaluation flow","text":"<p>Infer follows a clean and intuitive evaluation pipeline, illustrated by the workflow diagram you provided:</p> <p>EDA \u2192 Evaluation Metric \u2192 Confidence Interval \u2192 Output</p> <p>Let\u2019s break this down.</p>"},{"location":"blog/#1-exploratory-data-analysis-eda","title":"1. Exploratory Data Analysis (EDA)","text":"<p>Before computing any metric, Infer encourages understanding the data:</p> <ul> <li>Class balance for classification</li> <li>Distribution shape for regression targets</li> <li>Sample size and variance</li> <li>Potential sources of instability</li> </ul> <p>EDA is critical because CI method choice depends heavily on data properties. For example, small sample sizes or skewed distributions often invalidate simple Normal approximations.</p>"},{"location":"blog/#2-metric-computation","title":"2. Metric computation","text":"<p>Infer computes standard ML metrics using well-defined formulations. It supports a broad range of metrics across ML domains, including regression and classification metrics commonly used in practice. At this stage, Infer produces the familiar point estimate. This is necessary but not sufficient.</p>"},{"location":"blog/#3-confidence-interval-estimation","title":"3. Confidence interval estimation","text":"<p>This is where Infer differentiates itself.</p> <p>Infer integrates multiple CI estimation methods, including:</p> <ul> <li>Analytical methods like Wald and Wilson intervals for proportion-based metrics</li> <li>Exact and Bayesian-inspired intervals for small samples</li> <li>Bootstrap-based methods, including bias-corrected and accelerated bootstrap, for complex metrics and regression errors</li> <li>Specialized methods for metrics like ROC-AUC and cross-validation settings</li> </ul> <p>Each method has different assumptions, strengths, and failure modes. Infer documents these clearly and selects appropriate methods based on the metric and data characteristics.</p>"},{"location":"blog/#4-output-and-visualization","title":"4. Output and visualization","text":"<p>Infer does not stop at numbers. It generates:</p> <ul> <li>Metric values with confidence intervals</li> <li>Distribution plots from bootstrap samples</li> <li>CI bars and summaries for comparison</li> <li>Structured outputs suitable for reports and dashboards</li> </ul> <p>This makes uncertainty visible and actionable, not buried in logs.</p>"},{"location":"blog/#using-infer-for-classification-metrics","title":"Using Infer for classification metrics","text":"<p>Classification metrics are often proportions or composite statistics. Infer supports confidence intervals for metrics such as:</p> <ul> <li>Accuracy</li> <li>Precision and Recall</li> <li>F1-score</li> <li>ROC-AUC</li> </ul> <p>For example, in a binary classification experiment using Logistic Regression and XGBoost on the Breast Cancer Wisconsin dataset, Infer computed bootstrap confidence intervals for accuracy, precision, recall, and F1-score. The framework then validated whether true performance on a held-out validation set fell within the estimated CI ranges, achieving full coverage across all evaluated metrics.</p> <p>This kind of validation is essential. A CI that looks narrow but fails to cover the true value is misleading. Infer explicitly measures coverage quality, not just interval width.</p>"},{"location":"blog/#using-infer-for-regression-metrics","title":"Using Infer for regression metrics","text":"<p>Regression metrics often have complex, non-Normal distributions. Analytical CIs are usually unreliable here, which is why Infer emphasizes bootstrap methods for regression.</p> <p>In one real-world case study involving IV-drip flow rate prediction, Infer evaluated model performance using Mean Absolute Percentage Error (MAPE). The framework aggregated predictions and computed a bootstrap confidence interval around the MAPE estimate.</p> <p>The result:</p> <ul> <li>MAPE of 7.88%</li> <li>95% confidence interval of [6.47%, 9.84%]</li> <li>Corresponding accuracy interpretation of 92.12% with a well-defined uncertainty range</li> </ul> <p>This gives far more insight than a single error number. It tells you not just how well the model performed, but how stable that performance is under resampling and noise.</p>"},{"location":"blog/#why-infer-matters","title":"Why Infer matters","text":"<p>Infer is not just another metrics library. It represents a shift in how model evaluation is treated.</p> <p>Instead of asking:</p> <p>\u201cWhat is the metric?\u201d</p> <p>Infer encourages you to ask:</p> <p>\u201cHow confident are we in this metric?\u201d</p> <p>By combining rigorous statistical methods, practical ML metrics, and clear visual outputs, Infer helps teams:</p> <ul> <li>Make statistically defensible comparisons</li> <li>Avoid false improvements</li> <li>Communicate uncertainty clearly</li> <li>Build trust in ML systems</li> </ul> <p>In a world where ML models increasingly inform critical decisions, uncertainty is not optional. Infer makes it measurable, visible, and usable.</p>"},{"location":"dev-guide-metrics/","title":"Developer Guide: Adding New Metrics","text":"<p>This guide describes how to add new metrics to the confidence interval library. Follow these steps to contribute a new metric while maintaining consistency with the existing codebase.</p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#overview","title":"Overview","text":"<p>Adding a new metric requires updates to several files in the repository: - The appropriate metrics module (<code>regression_metrics.py</code> or <code>binary_metrics.py</code>) - <code>evaluator.py</code> (imports, docstrings, and mappings) - <code>__init__.py</code> (exports) - <code>README.md</code> (documentation)</p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-by-step-guide","title":"Step-by-Step Guide","text":"","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-1-define-your-metric-function","title":"Step 1: Define Your Metric Function","text":"<p>Add your metric function to the appropriate module: - Regression metrics: <code>regression_metrics.py</code> - Classification/Binary metrics: <code>binary_metrics.py</code></p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#required-function-signature","title":"Required Function Signature","text":"<p>Your metric function must follow this structure:</p> <pre><code>def your_metric_name(y_true: List[float],\n                     y_pred: List[float],\n                     confidence_level: float = 0.95,\n                     method: str = 'bootstrap_bca',\n                     compute_ci: bool = True,\n                     plot: bool = False,\n                     **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]]]:\n    \"\"\"\n    Compute [Your Metric Name] and optionally the confidence interval.\n\n    Parameters\n    ----------\n    y_true : List[float]\n        The ground truth target values.\n    y_pred : List[float]\n        The predicted target values.\n    confidence_level : float, optional\n        The confidence interval level, by default 0.95\n    method : str, optional\n        The bootstrap method, by default 'bootstrap_bca'\n    compute_ci : bool, optional\n        If true return the confidence interval as well as the metric score, by default True\n    plot : bool, optional\n        If true create histogram plot for bootstrap methods, by default False\n\n    Returns\n    -------\n    Union[float, Tuple[float, Tuple[float, float]]]\n        The metric score and optionally the confidence interval.\n    \"\"\"\n    def your_metric_calculation(y_true, y_pred):\n        # Implement your metric calculation here\n        # Example: return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n        pass\n\n    return _compute_metric_with_optional_ci(\n        y_true=y_true,\n        y_pred=y_pred,\n        metric_func=your_metric_calculation,\n        confidence_level=confidence_level,\n        method=method,\n        compute_ci=compute_ci,\n        plot=plot,\n        metric_name=\"your_metric_name\",\n        **kwargs\n    )\n</code></pre>","tags":["dev","development"]},{"location":"dev-guide-metrics/#example-mean-absolute-error-mae","title":"Example: Mean Absolute Error (MAE)","text":"<pre><code>def mae(y_true: List[float],\n        y_pred: List[float],\n        confidence_level: float = 0.95,\n        method: str = 'bootstrap_bca',\n        compute_ci: bool = True,\n        plot: bool = False,\n        **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]]]:\n    \"\"\"\n    Compute the Mean Absolute Error and optionally the confidence interval.\n\n    Parameters\n    ----------\n    y_true : List[float]\n        The ground truth target values.\n    y_pred : List[float]\n        The predicted target values.\n    confidence_level : float, optional\n        The confidence interval level, by default 0.95\n    method : str, optional\n        The bootstrap method, by default 'bootstrap_bca'\n    compute_ci : bool, optional\n        If true return the confidence interval as well as the MAE score, by default True\n    plot : bool, optional\n        If true create histogram plot for bootstrap methods, by default False\n\n    Returns\n    -------\n    Union[float, Tuple[float, Tuple[float, float]]]\n        The MAE score and optionally the confidence interval.\n    \"\"\"\n    def mae_metric(y_true, y_pred):\n        return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n\n    return _compute_metric_with_optional_ci(\n        y_true=y_true,\n        y_pred=y_pred,\n        metric_func=mae_metric,\n        confidence_level=confidence_level,\n        method=method,\n        compute_ci=compute_ci,\n        plot=plot,\n        metric_name=\"mae\",\n        **kwargs\n    )\n</code></pre>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-2-update-evaluatorpy-imports","title":"Step 2: Update <code>evaluator.py</code> Imports","text":"<p>Add your new metric to the imports section at the top of <code>evaluator.py</code>.</p> <p>For regression metrics: <pre><code># Import regression metrics\nfrom .regression_metrics import (\n    mae, mse, rmse, r2_score, mape, your_new_metric, regression_conf_methods\n)\n</code></pre></p> <p>For classification metrics: <pre><code># Import classification metrics\nfrom .binary_metrics import (\n    accuracy_score, ppv_score, tpr_score, your_new_metric\n)\n</code></pre></p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-3-update-evaluatorpy-class-docstring","title":"Step 3: Update <code>evaluator.py</code> Class Docstring","text":"<p>Add your metric to the <code>MetricEvaluator</code> class docstring so users can see all available metrics:</p> <pre><code>class MetricEvaluator:\n    \"\"\"\n    Unified evaluator for classification and regression metrics with confidence intervals.\n\n    This class provides a clean interface to compute various metrics with confidence intervals\n    for both classification and regression tasks without needing to import individual metrics.\n\n    Available Metrics:\n    ------------------\n    Classification:\n        - accuracy\n        - precision (PPV) \n        - recall (TPR / Sensitivity)\n        - specificity (TNR)\n        - npv (Negative Predictive Value)\n        - fpr (False Positive Rate)\n        - f1 (F1 Score, Takahashi method)\n        - precision_takahashi (Takahashi method)\n        - recall_takahashi (Takahashi method)\n        - auc (ROC AUC Score)\n        - your_new_classification_metric (Description)\n\n    Regression:\n        - mae (Mean Absolute Error)\n        - mse (Mean Squared Error)      \n        - rmse (Root Mean Squared Error)\n        - r2 (Coefficient of Determination)\n        - mape (Mean Absolute Percentage Error)\n        - your_new_regression_metric (Description)\n    \"\"\"\n</code></pre>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-4-update-evaluatorpy-metric-mappings","title":"Step 4: Update <code>evaluator.py</code> Metric Mappings","text":"<p>Add your metric to the appropriate dictionary in the <code>__init__</code> method of <code>MetricEvaluator</code>:</p> <p>For regression metrics:</p> <pre><code>def __init__(self):\n    \"\"\"Initialize the MetricEvaluator with available metrics and methods.\"\"\"\n\n    # Regression metrics mapping\n    self.regression_metrics = {\n        'mae': mae,\n        'mse': mse,\n        'rmse': rmse,\n        'r2': r2_score,\n        'mape': mape,\n        'your_new_metric': your_new_metric  # Add your metric here\n    }\n\n    # Expose individual metric functions as instance methods\n    self.mae = mae\n    self.mse = mse\n    self.rmse = rmse\n    self.r2_score = r2_score\n    self.mape = mape\n    self.your_new_metric = your_new_metric  # Add your metric here\n</code></pre> <p>For classification metrics: <pre><code>def __init__(self):\n    \"\"\"Initialize the MetricEvaluator with available metrics and methods.\"\"\"\n\n    # Classification metrics mapping\n    self.classification_metrics = {\n        'accuracy': accuracy_score,\n        'precision': ppv_score,\n        'recall': tpr_score,\n        'specificity': tnr_score,\n        'npv': npv_score,\n        'fpr': fpr_score,\n        'f1': f1_score,\n        'precision_takahashi': precision_score,\n        'recall_takahashi': recall_score,\n        'auc': roc_auc_score,\n        'your_new_metric': your_new_metric  # Add your metric here\n    }\n\n    # Expose individual metric functions as instance methods\n    self.accuracy_score = accuracy_score\n    self.ppv_score = ppv_score\n    # ... other metrics ...\n    self.your_new_metric = your_new_metric  # Add your metric here\n</code></pre></p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-5-update-__init__py-imports","title":"Step 5: Update <code>__init__.py</code> Imports","text":"<p>Add your metric to the import statements in <code>__init__.py</code>:</p> <p>For regression metrics: <pre><code>from .regression_metrics import mae, \\\n    mse, \\\n    rmse, \\\n    r2_score, \\\n    mape, \\\n    adjusted_r2_score, \\\n    sym_mean_abs_per_error, \\\n    rmse_log, \\\n    med_abs_err, \\\n    huber_loss, \\\n    exp_var_score, \\\n    mean_bia_dev, \\\n    your_new_metric  # Add your metric here\n</code></pre></p> <p>For classification metrics: <pre><code>from .binary_metrics import accuracy_score, \\\n    ppv_score, \\\n    npv_score, \\\n    tpr_score, \\\n    fpr_score, \\\n    tnr_score, \\\n    your_new_metric  # Add your metric here\n</code></pre></p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-6-update-__init__py-exports","title":"Step 6: Update <code>__init__.py</code> Exports","text":"<p>Add your metric to the <code>__all__</code> list in <code>__init__.py</code>:</p> <pre><code>__all__ = [\n    # Main unified interface\n    'MetricEvaluator', \n    'evaluate_metric',\n    'TaskType',\n\n    # Classification metrics\n    'accuracy_score', 'ppv_score', 'npv_score', 'tpr_score', \n    'fpr_score', 'tnr_score', 'precision_score', 'recall_score', \n    'f1_score', 'roc_auc_score', 'classification_report_with_ci',\n\n    # Regression metrics  \n    'mae', 'mse', 'rmse', 'r2_score', 'mape', 'adjusted_r2_score',\n    'sym_mean_abs_per_error', 'rmse_log', 'med_abs_err', 'huber_loss',\n    'exp_var_score', 'mean_bia_dev', 'your_new_metric',  # Add here\n\n    # Utility modules\n    'methods', 'utils'\n]\n</code></pre>","tags":["dev","development"]},{"location":"dev-guide-metrics/#step-7-update-readmemd-documentation","title":"Step 7: Update <code>README.md</code> Documentation","text":"<p>Update the README in two places:</p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#71-available-metrics-list","title":"7.1: Available Metrics List","text":"<p>Add your metric to the appropriate section:</p> <p>Classification: - <code>accuracy</code>: Overall classification accuracy - <code>precision</code>: Positive Predictive Value (PPV)  - <code>recall</code>: True Positive Rate/Sensitivity (TPR) - <code>specificity</code>: True Negative Rate (TNR) - <code>npv</code>: Negative Predictive Value - <code>fpr</code>: False Positive Rate - <code>f1</code>: F1 Score (supports averaging) - <code>precision_takahashi</code>: Precision using Takahashi method - <code>recall_takahashi</code>: Recall using Takahashi method - <code>auc</code>: ROC AUC Score - <code>your_new_metric</code>: Brief description of your metric</p> <p>Regression: - <code>mae</code>: Mean Absolute Error - <code>mse</code>: Mean Squared Error - <code>rmse</code>: Root Mean Squared Error - <code>r2</code>: Coefficient of Determination - <code>mape</code>: Mean Absolute Percentage Error - <code>your_new_metric</code>: Brief description of your metric</p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#72-usage-examples","title":"7.2: Usage Examples","text":"<p>Add a usage example in the appropriate section:</p> <p>For regression metrics: <pre><code>## Regression metrics\nfrom confidenceinterval import MetricEvaluator\n\nevaluate = MetricEvaluator()\n\nr2, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='r2', plot=True)\nmse, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='mse', plot=True)\nmae, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='mae', plot=True)\nrmse, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='rmse', plot=True)\nmape, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='mape', plot=True)\nyour_metric, ci = evaluate.evaluate(y_true, y_pred, task='regression', metric='your_new_metric', plot=True)\n</code></pre></p> <p>For classification metrics:</p>","tags":["dev","development"]},{"location":"dev-guide-metrics/#classification-metrics","title":"Classification metrics","text":"<pre><code>from confidenceinterval import MetricEvaluator\n\nevaluate = MetricEvaluator()\n\nf1, ci = evaluate.evaluate(y_true, y_pred, task='classification', metric='f1')\nprecision, ci = evaluate.evaluate(y_true, y_pred, task='classification', metric='precision')\nrecall, ci = evaluate.evaluate(y_true, y_pred, task='classification', metric='recall')\nyour_metric, ci = evaluate.evaluate(y_true, y_pred, task='classification', metric='your_new_metric')\n</code></pre>","tags":["dev","development"]},{"location":"dev-guide-metrics/#checklist","title":"Checklist","text":"<p>Before submitting your contribution, ensure you have:</p> <ul> <li> Defined the metric function in the appropriate module (<code>regression_metrics.py</code> or <code>binary_metrics.py</code>)</li> <li> Added imports to <code>evaluator.py</code></li> <li> Updated the <code>MetricEvaluator</code> class docstring in <code>evaluator.py</code></li> <li> Added the metric to the appropriate dictionary in <code>evaluator.py.__init__()</code></li> <li> Exposed the metric as an instance method in <code>evaluator.py.__init__()</code></li> <li> Added imports to <code>__init__.py</code></li> <li> Added the metric to <code>__all__</code> in <code>__init__.py</code></li> <li> Updated the available metrics list in <code>README.md</code></li> <li> Added usage examples to <code>README.md</code></li> <li> Tested your metric with sample data</li> <li> Ensured your function follows the required signature and uses <code>_compute_metric_with_optional_ci</code></li> </ul>","tags":["dev","development"]},{"location":"dev-guide-metrics/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Naming: Use clear, descriptive names that follow the existing naming conventions</li> <li>Complete Documentation: Include comprehensive docstrings with parameter descriptions and return types</li> <li>Type Hints: Use proper type hints for all function parameters and return values</li> <li>Error Handling: Ensure your metric handles edge cases appropriately</li> <li>Testing: Test your metric with various input scenarios before submitting</li> <li>Code Style: Follow the existing code style and formatting conventions</li> </ol>","tags":["dev","development"]},{"location":"dev-guide-metrics/#questions","title":"Questions?","text":"<p>If you have questions about contributing metrics, please open an issue in the repository.</p>","tags":["dev","development"]},{"location":"general_blog/","title":"Your model is not as good as you think or maybe it is. Without uncertainty, you cannot tell.","text":"<p>Machine learning teams love clean numbers: accuracy of 92 percent. RMSE of 3.4, F1 score of 0.88. These numbers feel objective, precise, and reassuring. They fit neatly into dashboards, slide decks, and go or no-go decisions.</p> <p></p> <p>But those values are not facts about the world. They are estimates derived from finite data, noisy labels, and a long chain of design choices such as sampling strategy, preprocessing, thresholds, and metric definitions. Treating a single number as definitive creates a false sense of certainty. That overconfidence often leads to poor decisions, unreliable deployments, missed regressions in minority groups, and wasted effort chasing noise during model selection.</p> <p>Before we talk about confidence intervals, we need to step back and talk about evaluation itself. Why do we evaluate models, what are metrics actually telling us, and where do they quietly fail.</p>"},{"location":"general_blog/#why-we-evaluate-machine-learning-models-at-all","title":"Why we evaluate machine learning models at all","text":"<p>Model evaluation exists to inform decisions about whether and how a machine learning system should be used. Training a model produces parameters that optimize a loss function on observed data, but evaluation is the step that connects that optimization process to real world deployment, where mistakes have costs and performance expectations must be met.</p> <p>In practice, evaluation helps teams determine whether a model meets the requirements of its intended use. It provides evidence about how the model behaves on data it was not trained on, how well it aligns with domain specific goals, and whether it satisfies constraints related to reliability, safety, or regulation. Without evaluation, there is no principled way to decide whether a model is ready to be deployed or how it compares to alternative approaches.</p> <p>Evaluation also serves as a shared reference point across teams. It allows engineers, researchers, product managers, and stakeholders to reason about model behavior using a common framework, even if their priorities differ. In this sense, evaluation is not only a technical step but also an organizational tool that supports accountability and informed decision making throughout the lifecycle of a machine learning system.</p>"},{"location":"general_blog/#metrics-are-abstractions-not-truths","title":"Metrics are abstractions, not truths","text":"<p>Evaluation metrics are simplified summaries of model behavior. Each metric captures one narrow notion of quality and ignores everything else.</p> <p>Consider accuracy in a customer churn model. If only 5 percent of customers churn, a model that predicts \u201cno churn\u201d for everyone achieves 95 percent accuracy while providing zero business value. The metric looks strong precisely because it ignores the asymmetric cost of errors and the rarity of the outcome the model is meant to detect.</p> <p>The same pattern appears across metrics. Accuracy assumes all errors have equal cost and that class distributions are stable. Precision and recall focus on different failure modes but ignore calibration and confidence. F1 score compresses competing objectives into a single value while hiding tradeoffs. Regression metrics like RMSE and MAE emphasize different error magnitudes and implicitly encode assumptions about loss functions.</p> <p>Choosing the right metric already requires domain understanding, cost modeling, and clarity about how predictions will be used. A fraud detection system, a medical diagnostic model, and a recommender system should not be evaluated the same way even if they use similar algorithms.</p>"},{"location":"general_blog/#the-hidden-assumption-behind-point-metrics","title":"The hidden assumption behind point metrics","text":"<p>When metric values are reported, they are often treated as fixed properties of a model, similar to architectural details such as the number of parameters or layers. Saying that a model has 92 percent accuracy or an F1 score of 0.88 implicitly suggests that these numbers are stable characteristics that can be relied on as precise summaries of performance.</p> <p>In reality, every reported metric is an estimate that depends on a specific evaluation setup. The value reflects the particular test sample that was drawn, the labels that were collected and possibly contain noise, and the operational choices embedded in the evaluation process, including preprocessing steps, thresholds, and metric definitions. None of these factors are fixed in practice, and small changes in any of them can alter the resulting metric.</p> <p>If the same model were evaluated on a slightly different test set drawn from the same underlying data distribution, the reported metric would almost certainly change. Sometimes the change would be minor, but in other cases it could be large enough to alter conclusions about model quality or readiness for deployment. Treating a single point estimate as definitive therefore relies on an implicit assumption that this variability is negligible, an assumption that often does not hold in real machine learning systems.</p>"},{"location":"general_blog/#where-single-metric-evaluation-fails-in-practice","title":"Where single metric evaluation fails in practice","text":"<p>The limitations of point metrics show up repeatedly across real world machine learning systems.</p>"},{"location":"general_blog/#small-or-medium-sized-test-sets","title":"Small or medium sized test sets","text":"<p>When test sets are small, randomness dominates. On a test set of a few hundred samples, one or two additional errors can materially change reported performance. Teams routinely interpret these fluctuations as meaningful improvements or regressions when they are simply noise.</p>"},{"location":"general_blog/#imbalanced-problems","title":"Imbalanced problems","text":"<p>In domains like fraud detection, medical diagnosis, or anomaly detection, the most important class is often the rare one. Metrics such as recall, precision, and F1 score can swing dramatically with small changes in class composition or labeling errors. A single point estimate can easily mask extreme instability in minority class performance.</p>"},{"location":"general_blog/#threshold-sensitivity","title":"Threshold sensitivity","text":"<p>Many metrics depend on a chosen decision threshold. A model that looks strong at one threshold may look weak at another. Reporting a single number hides how sensitive the model is to operational choices that will inevitably change over time.</p>"},{"location":"general_blog/#model-selection-and-hyperparameter-tuning","title":"Model selection and hyperparameter tuning","text":"<p>When dozens or hundreds of models are tried, the best looking metric is often the luckiest one. Selecting based on point estimates alone systematically favors noise. This is one of the most common sources of overestimated offline performance.</p>"},{"location":"general_blog/#subgroup-and-fairness-analysis","title":"Subgroup and fairness analysis","text":"<p>Aggregate metrics can look stable while subgroup performance is wildly uncertain. Small sample sizes within demographic slices amplify variance. Without explicitly accounting for uncertainty, teams often miss fragile or unsafe behavior in exactly the groups that matter most.</p>"},{"location":"general_blog/#the-real-question-metrics-do-not-answer","title":"The real question metrics do not answer","text":"<p>A single metric value answers a narrow and retrospective question: what did the model score on this particular evaluation sample?  That information is useful, but it is rarely sufficient for making real decisions.</p> <p>Most practical decisions depend on a forward looking question instead. How much could this metric plausibly change if the data were slightly different, if labels contained noise, or if operating conditions shifted in production? Without an answer to that question, teams cannot reliably judge whether an observed improvement is meaningful, whether a regression reflects real degradation, or whether a model is safe to deploy.</p> <p>This gap between reporting a number and understanding its stability is where many evaluation failures originate. Metrics summarize past performance, but decisions require an understanding of uncertainty. Confidence intervals exist precisely to bridge this gap, by turning point estimates into ranges that reflect how much trust we can place in them.</p>"},{"location":"general_blog/#from-point-estimates-to-uncertainty-aware-evaluation","title":"From point estimates to uncertainty aware evaluation","text":"<p>Confidence intervals transform evaluation from a static report into a probabilistic statement.</p> <p>Instead of saying the model has 92 percent accuracy, we say that the accuracy is likely to lie within a certain range given the data we observed. That range captures sampling variability, measurement noise, and sensitivity to evaluation choices.</p> <p>This shift changes how metrics are used. Metrics stop being treated as precise truths and start being treated as uncertain estimates. That single change has far reaching consequences for deployment decisions, monitoring, and model comparison.</p>"},{"location":"general_blog/#why-confidence-intervals-matter-in-machine-learning","title":"Why confidence intervals matter in machine learning","text":"<p>A single metric value summarizes what a model achieved on a specific evaluation sample, but it does not indicate how stable or reliable that result is. In practice, most decisions depend not only on the reported number but also on how much that number could change under slightly different conditions.</p> <p>Confidence intervals address this by quantifying the range within which a metric is likely to fall, given the inherent variability of finite data, noisy labels, and evaluation design choices. This is not a purely statistical concern. The width and position of an interval directly affect real decisions about deployment, monitoring, and prioritization.</p> <p></p> <p>By exposing best case and worst case scenarios, confidence intervals allow teams to translate abstract metrics into concrete business risk. They reduce the chance of selecting models that appear better only due to noise during hyperparameter tuning. They make it possible to distinguish expected metric fluctuation from genuine performance degradation after deployment. They also highlight instability in subgroups that aggregate metrics can hide, improving fairness analysis and regulatory confidence. Finally, confidence intervals improve communication by replacing overconfident point estimates with calibrated statements about what the data actually supports.</p>"},{"location":"general_blog/#confidence-intervals-in-real-decision-making","title":"Confidence intervals in real decision making","text":"<p>Consider a healthcare model that reports 94 percent sensitivity on a test set. On its own, that number sounds strong.</p> <p></p> <p>With a confidence interval, the picture changes. The sensitivity might plausibly lie between 87 percent and 98 percent. That lower bound may be unacceptable in a clinical setting where missed diagnoses carry serious consequences. Without the interval, that risk remains invisible.</p> <p>This logic is already standard in online experimentation. No serious A/B testing team would ship a change based on point estimates alone. Offline model evaluation is answering the same question: is this difference real, or is it noise?</p> <p>Confidence intervals bring the same discipline to machine learning evaluation.</p>"},{"location":"general_blog/#where-naive-confidence-interval-approaches-fail","title":"Where naive confidence interval approaches fail","text":"<p>Not all confidence intervals (CI) are created equal. Many common approaches break down in realistic ML settings.</p> <p>Metrics like accuracy and recall are not normally distributed in small samples. Bootstrapping can fail when data are highly imbalanced or correlated. Some metrics are discontinuous with respect to thresholds. Others depend on complex pipelines that violate independence assumptions.</p> <p>This is why confidence intervals need to be metric aware, data aware, and production ready. Treating CI computation as an afterthought often produces misleading results that are worse than no interval at all.</p>"},{"location":"general_blog/#introducing-infer","title":"Introducing Infer","text":"<p>Infer is a Python-based evaluation framework that turns point metrics into uncertainty-aware performance statements. Instead of returning a single score, Infer computes statistically appropriate confidence intervals, making model evaluation and comparison more reliable in real-world settings. </p>"},{"location":"general_blog/#what-makes-infer-different","title":"What makes Infer different","text":"<p>Infer is built on a simple insight: metrics without uncertainty quantification create statistical and business risk. Naive bootstrapping or default statistical methods often break silently when datasets are small, imbalanced, or when metrics violate their assumptions. Infer abstracts this complexity away.</p> <ul> <li> <p>Automatic CI-method selection:   Infer selects confidence-interval methods that match each metric\u2019s statistical properties, handling edge cases such as small samples, class imbalance, and zero-division safely.</p> </li> <li> <p>Metric-aware statistical assumptions:   Different metrics require different uncertainty models. Infer encodes these distinctions so practitioners do not need to reason about statistical validity metric by metric.</p> </li> <li> <p>Consistent, unified APIs:   The same interface works across classification, regression, and detection tasks, reducing evaluation friction and error-prone custom code.</p> </li> <li> <p>Reproducible evaluation outputs:   Structured results are designed for reports, dashboards, and stakeholder communication, not just ad-hoc experimentation.</p> </li> </ul>"},{"location":"general_blog/#installation","title":"Installation","text":""},{"location":"general_blog/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install infer-ci\n</code></pre>"},{"location":"general_blog/#from-source-development","title":"From Source (Development)","text":"<pre><code>git clone https://github.com/humblebeeai/infer-ci.git\ncd infer-ci\npip install -e \".[dev]\"\n</code></pre>"},{"location":"general_blog/#getting-started","title":"Getting started","text":"<pre><code># All the possible imports:\nfrom infer_ci import MetricEvaluator\nfrom infer_ci import accuracy_score, precision_score, tpr_score, f1_score\nfrom infer_ci import mae, mse, rmse, r2_score, iou\n\n# Classification example:\nevaluator = MetricEvaluator()\n\naccuracy, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='accuracy',\n    method='wilson'\n    # plot=True  # Supported in Bootstrap methods\n)\n</code></pre> <p>The result:</p> <ul> <li>MAPE of 7.88%</li> <li>95% confidence interval of [6.47%, 9.84%]</li> <li>Corresponding accuracy interpretation of 92.12% with a well-defined uncertainty range</li> </ul> <p>This gives far more insight than a single error number. It tells you not just how well the model performed, but how stable that performance is under resampling and noise.</p>"},{"location":"general_blog/#conclusion","title":"Conclusion","text":"<p>Machine learning evaluation is ultimately about decision making under uncertainty, yet most evaluation practices pretend that uncertainty does not exist. Point metrics give the illusion of precision while hiding the variability that determines whether a model improvement is real, reproducible, and safe to deploy. Confidence intervals restore that missing context by turning metrics into statements about reliability, risk, and stability rather than isolated numbers. Infer operationalizes this shift, making uncertainty-aware evaluation practical, consistent, and statistically sound across real-world ML workflows. In an era where models increasingly influence high-stakes decisions, treating uncertainty as optional is no longer defensible \u2014 and Infer makes it unavoidable, measurable, and actionable.</p>"},{"location":"object-detection-metrics-usage/","title":"Object Detection Metrics Usage Guide","text":"<p>This guide shows how to use the Object detection metrics with confidence intervals.</p>"},{"location":"object-detection-metrics-usage/#quick-start","title":"Quick Start","text":"<pre><code>from confidenceinterval import MetricEvaluator\nfrom ultralytics import YOLO\n\n# Initialize evaluator\nevaluate = MetricEvaluator()\n\n# Load your model and run predictions\nmodel = YOLO('yolov8n.pt')\nresults = model.predict(source='path/to/val-dataset/images', verbose=False)\n\n# Compute mAP@0.5:0.95 with confidence interval\nmap_value, (lower, upper) = evaluate.evaluate(\n    y_true='path/to/val-dataset',  # Path to dataset folder or data.yaml\n    y_pred=results,                 # prediction results\n    task='detection',\n    metric='map',\n    method='bootstrap_percentile',\n    n_resamples=1000,\n    plot=True                       # Create histogram plot\n)\n\nprint(f\"mAP@0.5:0.95: {map_value:.4f}\")\nprint(f\"95% CI: [{lower:.4f}, {upper:.4f}]\")\n</code></pre>"},{"location":"object-detection-metrics-usage/#available-detection-metrics","title":"Available Detection Metrics","text":"<ul> <li><code>map</code>: mAP@0.5:0.95 (mean Average Precision across IoU thresholds 0.5:0.05:0.95)</li> <li><code>map50</code>: mAP@0.5 (mean Average Precision at IoU threshold 0.5)</li> <li><code>precision</code>: Mean precision across all classes</li> <li><code>recall</code>: Mean recall across all classes</li> </ul>"},{"location":"object-detection-metrics-usage/#dataset-structure","title":"Dataset Structure","text":"<p>Your validation dataset should follow to this format:</p> <pre><code>val-dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 img001.jpg\n\u2502   \u251c\u2500\u2500 img002.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 img001.txt\n    \u251c\u2500\u2500 img002.txt\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Label files should contain one box per line in format: <pre><code>class_id x_center y_center width height\n</code></pre> Where all values are normalized to [0, 1].</p>"},{"location":"object-detection-metrics-usage/#per-class-metrics","title":"Per-Class Metrics","text":"<p>Get confidence intervals for each class:</p> <pre><code>map_value, (lower, upper) = evaluate.evaluate(\n    y_true='path/to/dataset',\n    y_pred=results,\n    task='detection',\n    metric='map',\n    plot_per_class=True,  # Enable per-class analysis\n    n_resamples=1000\n)\n\n# Output shows overall + per-class results:\n# Per-class mAP@0.5:0.95 with 95% Confidence Intervals:\n#      Class                          mAP@0.5:0.95 CI    Support\n#      all                            (0.922, 0.949)        1234\n#    0 person                         (0.900, 0.969)          45\n#    1 car                            (0.789, 0.955)          89\n#    ...\n</code></pre>"},{"location":"object-detection-metrics-usage/#using-different-metrics","title":"Using Different Metrics","text":"<pre><code># mAP@0.5\nmap50, ci = evaluate.evaluate(\n    y_true='path/to/dataset',\n    y_pred=results,\n    task='detection',\n    metric='map50',\n    n_resamples=1000\n)\n\n# Precision\nprecision, ci = evaluate.evaluate(\n    y_true='path/to/dataset',\n    y_pred=results,\n    task='detection',\n    metric='precision',\n    n_resamples=1000\n)\n\n# Recall\nrecall, ci = evaluate.evaluate(\n    y_true='path/to/val-dataset',\n    y_pred=results,\n    task='detection',\n    metric='recall',\n    n_resamples=1000\n)\n</code></pre>"},{"location":"object-detection-metrics-usage/#complete-example","title":"Complete Example","text":"<p>\ud83d\udcd3 For a complete interactive example with COCO128 dataset, see: <code>notebooks/example_detection.ipynb</code></p> <p>The notebook includes: - Step-by-step setup with COCO128 dataset</p>"},{"location":"object-detection-metrics-usage/#parameters","title":"Parameters","text":""},{"location":"object-detection-metrics-usage/#required","title":"Required","text":"<ul> <li><code>y_true</code>: Path to validation dataset directory or <code>data.yaml</code> file</li> <li><code>y_pred</code>: List of ultralytics Results objects from <code>model.predict()</code></li> </ul>"},{"location":"object-detection-metrics-usage/#optional","title":"Optional","text":"<ul> <li><code>confidence_level</code>: Confidence level (default: 0.95)</li> <li><code>method</code>: Bootstrap method (default: 'bootstrap_percentile')</li> <li><code>'bootstrap_percentile'</code>: Percentile method (prefered to use)</li> <li><code>'bootstrap_bca'</code>: Bias-corrected and accelerated (more accurate, slower)</li> <li><code>'bootstrap_basic'</code>: Basic bootstrap</li> <li><code>n_resamples</code>: Number of bootstrap resamples (default: 1000)</li> <li><code>random_state</code>: Random seed for reproducibility (default: 42)</li> <li><code>plot</code>: Create histogram plot for overall metric (default: False)</li> <li><code>plot_per_class</code>: Create separate plots for each class (default: False)</li> <li><code>compute_ci</code>: Return confidence interval (default: True)</li> </ul>"},{"location":"object-detection-metrics-usage/#notes","title":"Notes","text":"<ul> <li>Bootstrap resampling is done at the image level (resamples which images to include)</li> <li>No re-inference is needed; predictions are reused for each bootstrap iteration</li> <li>Per-class analysis computes all classes in a single bootstrap pass (50x faster than separate)</li> <li>COCO-standard AP calculation matches ultralytics/official COCO metrics exactly</li> <li>Works with any pip-installed <code>ultralytics</code> version</li> </ul>"},{"location":"object-detection-metrics-usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"object-detection-metrics-usage/#no-matching-ground-truth-found","title":"\"No matching ground truth found\"","text":"<ul> <li>Ensure image filenames match label filenames</li> <li>Check that labels are in <code>dataset/labels/</code> directory</li> <li>Label files should have <code>.txt</code> extension</li> </ul>"},{"location":"object-detection-metrics-usage/#no-label-files-found","title":"\"No label files found\"","text":"<ul> <li>Verify the dataset path is correct</li> <li>Ensure the directory structure follows above format</li> </ul>"},{"location":"object-detection-metrics-usage/#invalid-label-format","title":"\"Invalid label format\"","text":"<ul> <li>Check that label files have 5 columns: <code>class x_center y_center width height</code></li> <li>Ensure all values are normalized to [0, 1]</li> <li>Verify there are no extra spaces or invalid characters</li> </ul>"},{"location":"release-notes/","title":"\ud83d\udccc Release Notes","text":""},{"location":"about/authors/","title":"\ud83e\uddd9\u200d\u2642\ufe0f Authors","text":"<p>This project is developed by the following authors:</p> <ul> <li>@humblebeeai - HumbleBeeAI</li> </ul>","tags":["about"]},{"location":"about/contact/","title":"\ud83d\udcde Contact","text":"<p>You can contact me by email at services@humblebee.ai.</p>","tags":["about"]},{"location":"about/faq/","title":"\u2753 FAQ","text":"<p>This section contains frequently asked questions about this project.</p>","tags":["about"]},{"location":"about/faq/#q1-how-do-i-get-started-with-this-project","title":"Q1: How do I get started with this project?","text":"<p>To get started with this project, follow the instructions in the Getting Started guide.</p>","tags":["about"]},{"location":"about/license/","title":"\u00a9\ufe0f License","text":"<p>This project is licensed as the <code>LICENSE.md</code> file for details.</p>","tags":["about"]},{"location":"api-docs/","title":"API Reference","text":"<p>Welcome to the infer-ci API reference documentation. This section provides detailed information about all available metrics, methods, and utilities.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/#overview","title":"Overview","text":"<p>The infer-ci package provides a unified interface for computing machine learning evaluation metrics with confidence intervals. All metrics support multiple methods for calculating confidence intervals, including bootstrap, analytical, and jackknife methods.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/#core-interface","title":"Core Interface","text":"<ul> <li>MetricEvaluator: Unified interface for evaluating all metrics</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#metrics-by-category","title":"Metrics by Category","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/#classification-metrics","title":"Classification Metrics","text":"<p>Metrics for evaluating classification models:</p> <ul> <li>accuracy_score: Overall classification accuracy</li> <li>precision_score: Positive Predictive Value (PPV)</li> <li>recall_score: True Positive Rate/Sensitivity (TPR)</li> <li>f1_score: F1 Score (harmonic mean of precision and recall)</li> <li>ppv_score: Positive Predictive Value</li> <li>tpr_score: True Positive Rate</li> <li>tnr_score: True Negative Rate/Specificity</li> <li>npv_score: Negative Predictive Value</li> <li>fpr_score: False Positive Rate</li> <li>roc_auc_score: ROC AUC Score</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#regression-metrics","title":"Regression Metrics","text":"<p>Metrics for evaluating regression models:</p> <ul> <li>mae: Mean Absolute Error</li> <li>mse: Mean Squared Error</li> <li>rmse: Root Mean Squared Error</li> <li>r2_score: Coefficient of Determination (R\u00b2)</li> <li>mape: Mean Absolute Percentage Error</li> <li>iou: Intersection over Union</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#object-detection-metrics","title":"Object Detection Metrics","text":"<p>Metrics for evaluating object detection models:</p> <ul> <li>map: mean Average Precision @0.5:0.95</li> <li>map50: mean Average Precision @0.5</li> <li>precision: Detection Precision</li> <li>recall: Detection Recall</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#utilities","title":"Utilities","text":"<ul> <li>classification_report_with_ci: Generate classification report with confidence intervals</li> <li>CI Methods: Available confidence interval calculation methods</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#quick-start","title":"Quick Start","text":"<pre><code>from infer_ci import MetricEvaluator\n\n# Initialize evaluator\nevaluator = MetricEvaluator()\n\n# Evaluate a metric\nvalue, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='accuracy',\n    method='wilson'\n)\n\nprint(f\"Accuracy: {value:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/#common-parameters","title":"Common Parameters","text":"<p>Most metric functions share these common parameters:</p> <ul> <li><code>y_true</code> (array-like): Ground truth labels/values</li> <li><code>y_pred</code> (array-like): Predicted labels/values</li> <li><code>method</code> (str): Method for computing CI (default: 'bootstrap_bca')</li> <li><code>confidence_interval</code> (float): Confidence level between 0 and 1 (default: 0.95)</li> <li><code>n_resamples</code> (int): Number of bootstrap resamples (default: 2000)</li> <li><code>compute_ci</code> (bool): Whether to compute confidence interval (default: True)</li> <li><code>random_state</code> (int, optional): Random seed for reproducibility</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#available-ci-methods","title":"Available CI Methods","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/#bootstrap-methods","title":"Bootstrap Methods","text":"<ul> <li><code>bootstrap_bca</code>: Bootstrap Bias-Corrected and Accelerated (recommended)</li> <li><code>bootstrap_percentile</code>: Bootstrap Percentile method</li> <li><code>bootstrap_basic</code>: Basic Bootstrap method</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#analytical-methods-classification","title":"Analytical Methods (Classification)","text":"<ul> <li><code>wilson</code>: Wilson score interval</li> <li><code>normal</code>: Normal approximation</li> <li><code>agresti_coull</code>: Agresti-Coull interval</li> <li><code>beta</code>: Clopper-Pearson interval</li> <li><code>jeffreys</code>: Jeffreys interval</li> <li><code>binom_test</code>: Binomial test</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#other-methods","title":"Other Methods","text":"<ul> <li><code>jackknife</code>: Jackknife resampling method</li> <li><code>takahashi</code>: Takahashi method for F1, precision, recall (multi-class)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/#return-values","title":"Return Values","text":"<p>All metric functions return a tuple:</p> <pre><code>(metric_value, (lower_bound, upper_bound))\n</code></pre> <ul> <li><code>metric_value</code> (float): The computed metric value</li> <li><code>(lower_bound, upper_bound)</code> (tuple): The confidence interval bounds</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/","title":"MetricEvaluator","text":"<p>The <code>MetricEvaluator</code> class provides a unified interface for computing all metrics with confidence intervals.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator","title":"<code>infer_ci.evaluator.MetricEvaluator</code>","text":"<p>Unified evaluator for classification and regression metrics with confidence intervals.</p> <p>This class provides a clean interface to compute various metrics with confidence intervals for both classification and regression tasks without needing to import individual metrics.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator--available-metrics","title":"Available Metrics:","text":"<p>Classification:     - accuracy     - precision (PPV)     - recall (TPR / Sensitivity)     - specificity (TNR)     - npv (Negative Predictive Value)     - fpr (False Positive Rate)     - f1 (F1 Score, Takahashi method)     - precision_takahashi (Takahashi method)     - recall_takahashi (Takahashi method)     - auc (ROC AUC Score) Regression:     - mae (Mean Absolute Error)     - mse (Mean Squared Error)     - rmse (Root Mean Squared Error)     - r2 (Coefficient of Determination)     - mape (Mean Absolute Percentage Error)     - iou (Intersection over Union)</p> Detection <ul> <li>map (mAP@0.5:0.95 with CI)</li> <li>map50 (mAP@0.5 with CI)</li> <li>precision (Precision with CI)</li> <li>recall (Recall with CI)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator--example","title":"Example:","text":"<p>evaluator = MetricEvaluator()</p> Source code in <code>src/infer_ci/evaluator.py</code> <pre><code>class MetricEvaluator:\n    \"\"\"\n    Unified evaluator for classification and regression metrics with confidence intervals.\n\n    This class provides a clean interface to compute various metrics with confidence intervals\n    for both classification and regression tasks without needing to import individual metrics.\n\n    Available Metrics:\n    ------------------\n    Classification:\n        - accuracy\n        - precision (PPV)\n        - recall (TPR / Sensitivity)\n        - specificity (TNR)\n        - npv (Negative Predictive Value)\n        - fpr (False Positive Rate)\n        - f1 (F1 Score, Takahashi method)\n        - precision_takahashi (Takahashi method)\n        - recall_takahashi (Takahashi method)\n        - auc (ROC AUC Score)\n    Regression:\n        - mae (Mean Absolute Error)\n        - mse (Mean Squared Error)\n        - rmse (Root Mean Squared Error)\n        - r2 (Coefficient of Determination)\n        - mape (Mean Absolute Percentage Error)\n        - iou (Intersection over Union)\n\n    Detection:\n        - map (mAP@0.5:0.95 with CI)\n        - map50 (mAP@0.5 with CI)\n        - precision (Precision with CI)\n        - recall (Recall with CI)\n\n    Example:\n    --------\n    &gt;&gt;&gt; evaluator = MetricEvaluator()\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # For regression\n    &gt;&gt;&gt; result = evaluator.evaluate(\n    ...     y_true=[1, 2, 3, 4, 5], \n    ...     y_pred=[1.1, 2.2, 2.8, 4.1, 4.9],\n    ...     task='regression',\n    ...     metric='mae',\n    ...     method='bootstrap_bca',\n    ...     plot=True\n    ... ) \n    ... # Specific metric function can also be called directly:  \n    &gt;&gt;&gt; mae_val, ci = evaluator.mae(\n    ...     y_true=[1, 2, 3, 4, 5], \n    ...     y_pred=[1.1, 2.2, 2.8, 4.1, 4.9],\n    ...     confidence_level=0.95,\n    ...     method='jackknife'\n    ... )\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # For classification  \n    &gt;&gt;&gt; result = evaluator.evaluate(\n    ...     y_true=[0, 1, 1, 0, 1], \n    ...     y_pred=[0, 1, 0, 0, 1],\n    ...     task='classification',\n    ...     metric='accuracy',\n    ...     method='wilson'\n    ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the MetricEvaluator with available metrics and methods.\"\"\"\n\n        # Classification metrics mapping\n        self.classification_metrics = {\n            'accuracy': accuracy_score,\n            'precision': ppv_score,  # Positive Predictive Value\n            'recall': tpr_score,     # True Positive Rate / Sensitivity\n            'specificity': tnr_score, # True Negative Rate\n            'npv': npv_score,        # Negative Predictive Value\n            'fpr': fpr_score,        # False Positive Rate\n            'f1': f1_score,          # F1 Score (Takahashi method)\n            'precision_takahashi': precision_score,  # Takahashi precision\n            'recall_takahashi': recall_score,        # Takahashi recall\n            'auc': roc_auc_score     # ROC AUC Score\n        }\n\n        # Regression metrics mapping\n        self.regression_metrics = {\n            'mae': mae,           # Mean Absolute Error\n            'mse': mse,           # Mean Squared Error\n            'rmse': rmse,         # Root Mean Squared Error\n            'r2': r2_score,       # Coefficient of Determination\n            'mape': mape,         # Mean Absolute Percentage Error\n            'iou': iou            # Intersection over Union\n        }\n\n        # Detection metrics mapping\n        self.detection_metrics = {\n            'map': map,               # mAP@0.5:0.95 with CI\n            'map50': map50,           # mAP@0.5 with CI\n            'precision': precision,   # Precision with CI\n            'recall': recall          # Recall with CI\n        }\n\n        # Available methods for each task type\n        self.classification_methods = proportion_conf_methods + ['bootstrap_bca', 'bootstrap_percentile', 'bootstrap_basic']\n        self.regression_methods = regression_conf_methods\n        # Detection uses bootstrap methods\n        self.detection_methods = ['bootstrap_bca', 'bootstrap_percentile', 'bootstrap_basic']\n\n        # Expose individual metric functions as instance methods\n        # Classification metrics\n        self.accuracy_score = accuracy_score\n        self.ppv_score = ppv_score\n        self.tpr_score = tpr_score\n        self.tnr_score = tnr_score\n        self.fpr_score = fpr_score\n        self.npv_score = npv_score\n        self.f1_score = f1_score\n        self.precision_score = precision_score\n        self.recall_score = recall_score\n        self.roc_auc_score = roc_auc_score\n\n        # Regression metrics\n        self.mae = mae\n        self.mse = mse\n        self.rmse = rmse\n        self.r2_score = r2_score\n        self.mape = mape\n        self.iou = iou\n\n        # Detection metrics\n        self.map = map\n        self.map50 = map50\n        self.precision = precision\n        self.recall = recall\n\n    def get_available_metrics(self, task: Union[str, TaskType]) -&gt; List[str]:\n        \"\"\"\n        Get list of available metrics for a given task type.\n\n        Parameters:\n        -----------\n        task : str or TaskType\n            The task type ('classification' or 'regression')\n\n        Returns:\n        --------\n        List[str]: List of available metric names\n        \"\"\"\n        task_str = task.value if isinstance(task, TaskType) else task.lower()\n\n        if task_str == 'classification':\n            return list(self.classification_metrics.keys())\n        elif task_str == 'regression':\n            return list(self.regression_metrics.keys())\n        elif task_str == 'detection':\n            return list(self.detection_metrics.keys())\n        else:\n            raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n\n    def get_available_methods(self, task: Union[str, TaskType]) -&gt; List[str]:\n        \"\"\"\n        Get list of available confidence interval methods for a given task type.\n\n        Parameters:\n        -----------\n        task : str or TaskType\n            The task type ('classification' or 'regression')\n\n        Returns:\n        --------\n        List[str]: List of available method names\n        \"\"\"\n        task_str = task.value if isinstance(task, TaskType) else task.lower()\n\n        if task_str == 'classification':\n            return self.classification_methods\n        elif task_str == 'regression':\n            return self.regression_methods\n        elif task_str == 'detection':\n            return self.detection_methods\n        else:\n            raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n\n    def evaluate(self,\n                 y_true: Optional[List[Union[int, float]]] = None,\n                 y_pred: Optional[List[Union[int, float]]] = None,\n                 task: Union[str, TaskType] = None,\n                 metric: str = None,\n                 method: str = 'bootstrap_bca',\n                 confidence_level: float = 0.95,\n                 compute_ci: bool = True,\n                 plot: bool = False,\n                 model: Optional[Any] = None,\n                 data: Optional[str] = None,\n                 **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]], Dict[str, Tuple[float, Tuple[float, float]]]]:\n        \"\"\"\n        Evaluate a metric with confidence intervals.\n\n        Parameters:\n        -----------\n        y_true : List[Union[int, float]], optional\n            True labels/values (required for classification and regression tasks)\n        y_pred : List[Union[int, float]], optional\n            Predicted labels/values (required for classification and regression tasks)\n        task : str or TaskType\n            Task type ('classification', 'regression', or 'detection')\n        metric : str\n            Metric name (e.g., 'accuracy', 'mae', 'f1', 'map')\n        method : str, optional\n            Confidence interval method (default: 'bootstrap_bca')\n        confidence_level : float, optional\n            Confidence level (default: 0.95)\n        compute_ci : bool, optional\n            Whether to compute confidence intervals (default: True)\n        plot : bool, optional\n            Whether to create histogram plot for bootstrap methods (default: False)\n        model : Any, optional\n            Model object or path (required for detection tasks)\n        data : str, optional\n            Dataset configuration path (required for detection tasks)\n        **kwargs : dict\n            Additional arguments (e.g., n_resamples for bootstrap methods,\n            n_iterations for detection tasks)\n\n        Returns:\n        --------\n        Union[float, Tuple[float, Tuple[float, float]], Dict[str, Tuple[float, Tuple[float, float]]]]\n            For classification/regression:\n                If compute_ci=False: metric value\n                If compute_ci=True: (metric_value, (lower_bound, upper_bound))\n            For detection:\n                Dictionary with class names as keys and (mean_metric, (lower_CI, upper_CI)) as values\n\n        Example:\n        --------\n        &gt;&gt;&gt; evaluator = MetricEvaluator()\n        &gt;&gt;&gt; # Regression example\n        &gt;&gt;&gt; mae_val, ci = evaluator.evaluate(\n        ...     y_true=[1, 2, 3, 4, 5],\n        ...     y_pred=[1.1, 2.1, 2.9, 4.1, 4.9],\n        ...     task='regression',\n        ...     metric='mae',\n        ...     method='jackknife'\n        ... )\n        &gt;&gt;&gt; print(f\"MAE: {mae_val:.3f}, 95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detection example\n        &gt;&gt;&gt; from ultralytics import YOLO\n        &gt;&gt;&gt; model = YOLO('yolov8n.pt')\n        &gt;&gt;&gt; results = model.predict(source='dataset/images')\n        &gt;&gt;&gt; map_val, (lower, upper) = evaluator.evaluate(\n        ...     y_true='dataset',\n        ...     y_pred=results,\n        ...     task='detection',\n        ...     metric='map',\n        ...     method='bootstrap_percentile',\n        ...     n_resamples=1000\n        ... )\n        &gt;&gt;&gt; print(f\"mAP@0.5:0.95: {map_val:.3f} [{lower:.3f}, {upper:.3f}]\")\n        \"\"\"\n\n        # Validate and normalize task type\n        task_str = task.value if isinstance(task, TaskType) else task.lower()\n        if task_str not in ['classification', 'regression', 'detection']:\n            raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n\n        # Handle detection tasks separately (different interface)\n        if task_str == 'detection':\n            # Detection tasks require y_true (dataset path) and y_pred (Results objects)\n            if y_true is None or y_pred is None:\n                raise ValueError(\n                    \"Detection tasks require:\\n\"\n                    \"  - 'y_true': Path to validation dataset directory (e.g., 'val-dataset')\\n\"\n                    \"  - 'y_pred': List of ultralytics Results objects from model.predict()\"\n                )\n\n            if metric not in self.detection_metrics:\n                available = ', '.join(self.detection_metrics.keys())\n                raise ValueError(f\"Unknown detection metric: {metric}. Available: {available}\")\n\n            metric_func = self.detection_metrics[metric]\n\n            # Call the detection metric function\n            try:\n                result = metric_func(\n                    y_true=y_true,\n                    y_pred=y_pred,\n                    confidence_level=confidence_level,\n                    method=method,\n                    compute_ci=compute_ci,\n                    plot=plot,\n                    **kwargs\n                )\n                return result\n\n            except Exception as e:\n                raise RuntimeError(f\"Error computing {metric}: {str(e)}\")\n\n        # Handle classification and regression tasks\n        if y_true is None or y_pred is None:\n            raise ValueError(\"Classification and regression tasks require 'y_true' and 'y_pred' parameters\")\n\n        # Get the appropriate metric function and validate\n        if task_str == 'classification':\n            if metric not in self.classification_metrics:\n                available = ', '.join(self.classification_metrics.keys())\n                raise ValueError(f\"Unknown classification metric: {metric}. Available: {available}\")\n            metric_func = self.classification_metrics[metric]\n\n            # Validate method for classification\n            if method not in self.classification_methods:\n                available = ', '.join(self.classification_methods)\n                raise ValueError(f\"Unknown classification method: {method}. Available: {available}\")\n\n        else:  # regression\n            if metric not in self.regression_metrics:\n                available = ', '.join(self.regression_metrics.keys())\n                raise ValueError(f\"Unknown regression metric: {metric}. Available: {available}\")\n            metric_func = self.regression_metrics[metric]\n\n            # Validate method for regression\n            if method not in self.regression_methods:\n                available = ', '.join(self.regression_methods)\n                raise ValueError(f\"Unknown regression method: {method}. Available: {available}\")\n\n        # Set default kwargs for different methods\n        if method.startswith('bootstrap') and 'n_resamples' not in kwargs:\n            kwargs['n_resamples'] = 9999\n\n        # Call the metric function\n        try:\n            result = metric_func(\n                y_true=y_true,\n                y_pred=y_pred,\n                confidence_level=confidence_level,\n                method=method,\n                compute_ci=compute_ci,\n                plot=plot,\n                **kwargs\n            )\n            return result\n\n        except Exception as e:\n            raise RuntimeError(f\"Error computing {metric} with method {method}: {str(e)}\")\n\n    def evaluate_multiple(self,\n                         y_true: List[Union[int, float]], \n                         y_pred: List[Union[int, float]], \n                         task: Union[str, TaskType],\n                         metrics: List[str],\n                         method: str = 'bootstrap_bca',\n                         confidence_level: float = 0.95,\n                         compute_ci: bool = True,\n                         plot: bool = False,\n                         **kwargs) -&gt; Dict[str, Union[float, Tuple[float, Tuple[float, float]]]]:\n        \"\"\"\n        Evaluate multiple metrics at once.\n\n        Parameters:\n        -----------\n        y_true, y_pred, task, method, confidence_level, compute_ci, plot, **kwargs\n            Same as evaluate() method\n        metrics : List[str]\n            List of metric names to evaluate\n\n        Returns:\n        --------\n        Dict[str, Union[float, Tuple[float, Tuple[float, float]]]]\n            Dictionary with metric names as keys and results as values\n\n        Example:\n        --------\n        &gt;&gt;&gt; evaluator = MetricEvaluator()\n        &gt;&gt;&gt; results = evaluator.evaluate_multiple(\n        ...     y_true=[1, 2, 3, 4, 5],\n        ...     y_pred=[1.1, 2.1, 2.9, 4.1, 4.9],\n        ...     task='regression',\n        ...     metrics=['mae', 'rmse', 'r2'],\n        ...     method='jackknife'\n        ... )\n        &gt;&gt;&gt; for metric, (value, ci) in results.items():\n        ...     print(f\"{metric}: {value:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n        \"\"\"\n        results = {}\n        for metric in metrics:\n            results[metric] = self.evaluate(\n                y_true=y_true,\n                y_pred=y_pred,\n                task=task,\n                metric=metric,\n                method=method,\n                confidence_level=confidence_level,\n                compute_ci=compute_ci,\n                plot=plot,\n                **kwargs\n            )\n        return results\n\n    def info(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get information about available tasks, metrics, and methods.\n\n        Returns:\n        --------\n        Dict[str, Any]: Dictionary containing all available options\n        \"\"\"\n        return {\n            'tasks': ['classification', 'regression', 'detection'],\n            'classification': {\n                'metrics': list(self.classification_metrics.keys()),\n                'methods': self.classification_methods\n            },\n            'regression': {\n                'metrics': list(self.regression_metrics.keys()),\n                'methods': self.regression_methods\n            },\n            'detection': {\n                'metrics': list(self.detection_metrics.keys()),\n                'methods': self.detection_methods\n            }\n        }\n\n    def print_info(self):\n        \"\"\"Print formatted information about available options.\"\"\"\n        print(\"MetricEvaluator - Available Options\")\n        print(\"=\" * 40)\n\n        print(\"\\n\ud83d\udcca CLASSIFICATION METRICS:\")\n        for metric in self.classification_metrics.keys():\n            print(f\"   - {metric}\")\n\n        print(f\"\\n\ud83d\udd27 Classification Methods: {len(self.classification_methods)}\")\n        for method in self.classification_methods:\n            print(f\"   - {method}\")\n\n        print(\"\\n\ud83d\udcc8 REGRESSION METRICS:\")\n        for metric in self.regression_metrics.keys():\n            print(f\"   - {metric}\")\n\n        print(f\"\\n\ud83d\udd27 Regression Methods: {len(self.regression_methods)}\")\n        for method in self.regression_methods:\n            print(f\"   - {method}\")\n\n        print(\"\\n\ud83c\udfaf DETECTION METRICS:\")\n        for metric in self.detection_metrics.keys():\n            print(f\"   - {metric}\")\n\n        print(f\"\\n\ud83d\udd27 Detection Methods: {len(self.detection_methods)}\")\n        for method in self.detection_methods:\n            print(f\"   - {method}\")\n\n        print(f\"\\nDefault method: bootstrap_bca\")\n        print(f\"Default confidence level: 0.95\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator--for-regression","title":"For regression","text":"<p>result = evaluator.evaluate( ...     y_true=[1, 2, 3, 4, 5],  ...     y_pred=[1.1, 2.2, 2.8, 4.1, 4.9], ...     task='regression', ...     metric='mae', ...     method='bootstrap_bca', ...     plot=True ... )  ... # Specific metric function can also be called directly: mae_val, ci = evaluator.mae( ...     y_true=[1, 2, 3, 4, 5],  ...     y_pred=[1.1, 2.2, 2.8, 4.1, 4.9], ...     confidence_level=0.95, ...     method='jackknife' ... )</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator--for-classification","title":"For classification","text":"<p>result = evaluator.evaluate( ...     y_true=[0, 1, 1, 0, 1],  ...     y_pred=[0, 1, 0, 0, 1], ...     task='classification', ...     metric='accuracy', ...     method='wilson' ... )</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate","title":"<code>evaluate(y_true=None, y_pred=None, task=None, metric=None, method='bootstrap_bca', confidence_level=0.95, compute_ci=True, plot=False, model=None, data=None, **kwargs)</code>","text":"<p>Evaluate a metric with confidence intervals.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate--parameters","title":"Parameters:","text":"<p>y_true : List[Union[int, float]], optional     True labels/values (required for classification and regression tasks) y_pred : List[Union[int, float]], optional     Predicted labels/values (required for classification and regression tasks) task : str or TaskType     Task type ('classification', 'regression', or 'detection') metric : str     Metric name (e.g., 'accuracy', 'mae', 'f1', 'map') method : str, optional     Confidence interval method (default: 'bootstrap_bca') confidence_level : float, optional     Confidence level (default: 0.95) compute_ci : bool, optional     Whether to compute confidence intervals (default: True) plot : bool, optional     Whether to create histogram plot for bootstrap methods (default: False) model : Any, optional     Model object or path (required for detection tasks) data : str, optional     Dataset configuration path (required for detection tasks) **kwargs : dict     Additional arguments (e.g., n_resamples for bootstrap methods,     n_iterations for detection tasks)</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate--returns","title":"Returns:","text":"<p>Union[float, Tuple[float, Tuple[float, float]], Dict[str, Tuple[float, Tuple[float, float]]]]     For classification/regression:         If compute_ci=False: metric value         If compute_ci=True: (metric_value, (lower_bound, upper_bound))     For detection:         Dictionary with class names as keys and (mean_metric, (lower_CI, upper_CI)) as values</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate--example","title":"Example:","text":"<p>evaluator = MetricEvaluator()</p> Source code in <code>src/infer_ci/evaluator.py</code> <pre><code>def evaluate(self,\n             y_true: Optional[List[Union[int, float]]] = None,\n             y_pred: Optional[List[Union[int, float]]] = None,\n             task: Union[str, TaskType] = None,\n             metric: str = None,\n             method: str = 'bootstrap_bca',\n             confidence_level: float = 0.95,\n             compute_ci: bool = True,\n             plot: bool = False,\n             model: Optional[Any] = None,\n             data: Optional[str] = None,\n             **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]], Dict[str, Tuple[float, Tuple[float, float]]]]:\n    \"\"\"\n    Evaluate a metric with confidence intervals.\n\n    Parameters:\n    -----------\n    y_true : List[Union[int, float]], optional\n        True labels/values (required for classification and regression tasks)\n    y_pred : List[Union[int, float]], optional\n        Predicted labels/values (required for classification and regression tasks)\n    task : str or TaskType\n        Task type ('classification', 'regression', or 'detection')\n    metric : str\n        Metric name (e.g., 'accuracy', 'mae', 'f1', 'map')\n    method : str, optional\n        Confidence interval method (default: 'bootstrap_bca')\n    confidence_level : float, optional\n        Confidence level (default: 0.95)\n    compute_ci : bool, optional\n        Whether to compute confidence intervals (default: True)\n    plot : bool, optional\n        Whether to create histogram plot for bootstrap methods (default: False)\n    model : Any, optional\n        Model object or path (required for detection tasks)\n    data : str, optional\n        Dataset configuration path (required for detection tasks)\n    **kwargs : dict\n        Additional arguments (e.g., n_resamples for bootstrap methods,\n        n_iterations for detection tasks)\n\n    Returns:\n    --------\n    Union[float, Tuple[float, Tuple[float, float]], Dict[str, Tuple[float, Tuple[float, float]]]]\n        For classification/regression:\n            If compute_ci=False: metric value\n            If compute_ci=True: (metric_value, (lower_bound, upper_bound))\n        For detection:\n            Dictionary with class names as keys and (mean_metric, (lower_CI, upper_CI)) as values\n\n    Example:\n    --------\n    &gt;&gt;&gt; evaluator = MetricEvaluator()\n    &gt;&gt;&gt; # Regression example\n    &gt;&gt;&gt; mae_val, ci = evaluator.evaluate(\n    ...     y_true=[1, 2, 3, 4, 5],\n    ...     y_pred=[1.1, 2.1, 2.9, 4.1, 4.9],\n    ...     task='regression',\n    ...     metric='mae',\n    ...     method='jackknife'\n    ... )\n    &gt;&gt;&gt; print(f\"MAE: {mae_val:.3f}, 95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Detection example\n    &gt;&gt;&gt; from ultralytics import YOLO\n    &gt;&gt;&gt; model = YOLO('yolov8n.pt')\n    &gt;&gt;&gt; results = model.predict(source='dataset/images')\n    &gt;&gt;&gt; map_val, (lower, upper) = evaluator.evaluate(\n    ...     y_true='dataset',\n    ...     y_pred=results,\n    ...     task='detection',\n    ...     metric='map',\n    ...     method='bootstrap_percentile',\n    ...     n_resamples=1000\n    ... )\n    &gt;&gt;&gt; print(f\"mAP@0.5:0.95: {map_val:.3f} [{lower:.3f}, {upper:.3f}]\")\n    \"\"\"\n\n    # Validate and normalize task type\n    task_str = task.value if isinstance(task, TaskType) else task.lower()\n    if task_str not in ['classification', 'regression', 'detection']:\n        raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n\n    # Handle detection tasks separately (different interface)\n    if task_str == 'detection':\n        # Detection tasks require y_true (dataset path) and y_pred (Results objects)\n        if y_true is None or y_pred is None:\n            raise ValueError(\n                \"Detection tasks require:\\n\"\n                \"  - 'y_true': Path to validation dataset directory (e.g., 'val-dataset')\\n\"\n                \"  - 'y_pred': List of ultralytics Results objects from model.predict()\"\n            )\n\n        if metric not in self.detection_metrics:\n            available = ', '.join(self.detection_metrics.keys())\n            raise ValueError(f\"Unknown detection metric: {metric}. Available: {available}\")\n\n        metric_func = self.detection_metrics[metric]\n\n        # Call the detection metric function\n        try:\n            result = metric_func(\n                y_true=y_true,\n                y_pred=y_pred,\n                confidence_level=confidence_level,\n                method=method,\n                compute_ci=compute_ci,\n                plot=plot,\n                **kwargs\n            )\n            return result\n\n        except Exception as e:\n            raise RuntimeError(f\"Error computing {metric}: {str(e)}\")\n\n    # Handle classification and regression tasks\n    if y_true is None or y_pred is None:\n        raise ValueError(\"Classification and regression tasks require 'y_true' and 'y_pred' parameters\")\n\n    # Get the appropriate metric function and validate\n    if task_str == 'classification':\n        if metric not in self.classification_metrics:\n            available = ', '.join(self.classification_metrics.keys())\n            raise ValueError(f\"Unknown classification metric: {metric}. Available: {available}\")\n        metric_func = self.classification_metrics[metric]\n\n        # Validate method for classification\n        if method not in self.classification_methods:\n            available = ', '.join(self.classification_methods)\n            raise ValueError(f\"Unknown classification method: {method}. Available: {available}\")\n\n    else:  # regression\n        if metric not in self.regression_metrics:\n            available = ', '.join(self.regression_metrics.keys())\n            raise ValueError(f\"Unknown regression metric: {metric}. Available: {available}\")\n        metric_func = self.regression_metrics[metric]\n\n        # Validate method for regression\n        if method not in self.regression_methods:\n            available = ', '.join(self.regression_methods)\n            raise ValueError(f\"Unknown regression method: {method}. Available: {available}\")\n\n    # Set default kwargs for different methods\n    if method.startswith('bootstrap') and 'n_resamples' not in kwargs:\n        kwargs['n_resamples'] = 9999\n\n    # Call the metric function\n    try:\n        result = metric_func(\n            y_true=y_true,\n            y_pred=y_pred,\n            confidence_level=confidence_level,\n            method=method,\n            compute_ci=compute_ci,\n            plot=plot,\n            **kwargs\n        )\n        return result\n\n    except Exception as e:\n        raise RuntimeError(f\"Error computing {metric} with method {method}: {str(e)}\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate--regression-example","title":"Regression example","text":"<p>mae_val, ci = evaluator.evaluate( ...     y_true=[1, 2, 3, 4, 5], ...     y_pred=[1.1, 2.1, 2.9, 4.1, 4.9], ...     task='regression', ...     metric='mae', ...     method='jackknife' ... ) print(f\"MAE: {mae_val:.3f}, 95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.evaluate--detection-example","title":"Detection example","text":"<p>from ultralytics import YOLO model = YOLO('yolov8n.pt') results = model.predict(source='dataset/images') map_val, (lower, upper) = evaluator.evaluate( ...     y_true='dataset', ...     y_pred=results, ...     task='detection', ...     metric='map', ...     method='bootstrap_percentile', ...     n_resamples=1000 ... ) print(f\"mAP@0.5:0.95: {map_val:.3f} [{lower:.3f}, {upper:.3f}]\")</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_metrics","title":"<code>get_available_metrics(task)</code>","text":"<p>Get list of available metrics for a given task type.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_metrics--parameters","title":"Parameters:","text":"<p>task : str or TaskType     The task type ('classification' or 'regression')</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_metrics--returns","title":"Returns:","text":"<p>List[str]: List of available metric names</p> Source code in <code>src/infer_ci/evaluator.py</code> <pre><code>def get_available_metrics(self, task: Union[str, TaskType]) -&gt; List[str]:\n    \"\"\"\n    Get list of available metrics for a given task type.\n\n    Parameters:\n    -----------\n    task : str or TaskType\n        The task type ('classification' or 'regression')\n\n    Returns:\n    --------\n    List[str]: List of available metric names\n    \"\"\"\n    task_str = task.value if isinstance(task, TaskType) else task.lower()\n\n    if task_str == 'classification':\n        return list(self.classification_metrics.keys())\n    elif task_str == 'regression':\n        return list(self.regression_metrics.keys())\n    elif task_str == 'detection':\n        return list(self.detection_metrics.keys())\n    else:\n        raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_methods","title":"<code>get_available_methods(task)</code>","text":"<p>Get list of available confidence interval methods for a given task type.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_methods--parameters","title":"Parameters:","text":"<p>task : str or TaskType     The task type ('classification' or 'regression')</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#infer_ci.evaluator.MetricEvaluator.get_available_methods--returns","title":"Returns:","text":"<p>List[str]: List of available method names</p> Source code in <code>src/infer_ci/evaluator.py</code> <pre><code>def get_available_methods(self, task: Union[str, TaskType]) -&gt; List[str]:\n    \"\"\"\n    Get list of available confidence interval methods for a given task type.\n\n    Parameters:\n    -----------\n    task : str or TaskType\n        The task type ('classification' or 'regression')\n\n    Returns:\n    --------\n    List[str]: List of available method names\n    \"\"\"\n    task_str = task.value if isinstance(task, TaskType) else task.lower()\n\n    if task_str == 'classification':\n        return self.classification_methods\n    elif task_str == 'regression':\n        return self.regression_methods\n    elif task_str == 'detection':\n        return self.detection_methods\n    else:\n        raise ValueError(f\"Unknown task type: {task}. Use 'classification', 'regression', or 'detection'\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#overview","title":"Overview","text":"<p><code>MetricEvaluator</code> is the recommended way to compute metrics in infer-ci. It provides a consistent interface across all metric types (classification, regression, detection) and handles method selection automatically.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#basic-usage","title":"Basic Usage","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\n# Initialize evaluator\nevaluator = MetricEvaluator()\n\n# Binary classification example\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1])\n\n# Compute accuracy with Wilson interval\naccuracy, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='accuracy',\n    method='wilson'\n)\n\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#supported-tasks-and-metrics","title":"Supported Tasks and Metrics","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#classification-task","title":"Classification Task","text":"<pre><code># Get all available classification metrics\nmetrics = evaluator.get_available_metrics('classification')\nprint(metrics)\n# ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'npv', 'fpr', 'auc', ...]\n</code></pre> <p>Available metrics: - <code>accuracy</code>: Overall accuracy - <code>precision</code>: Positive Predictive Value - <code>recall</code>: True Positive Rate/Sensitivity - <code>f1</code>: F1 Score (supports binary, macro, micro averaging) - <code>specificity</code>: True Negative Rate - <code>npv</code>: Negative Predictive Value - <code>fpr</code>: False Positive Rate - <code>auc</code>: ROC AUC Score - <code>precision_takahashi</code>: Precision with Takahashi method - <code>recall_takahashi</code>: Recall with Takahashi method</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#regression-task","title":"Regression Task","text":"<pre><code># Get all available regression metrics\nmetrics = evaluator.get_available_metrics('regression')\nprint(metrics)\n# ['mae', 'mse', 'rmse', 'r2', 'mape', 'iou']\n</code></pre> <p>Available metrics: - <code>mae</code>: Mean Absolute Error - <code>mse</code>: Mean Squared Error - <code>rmse</code>: Root Mean Squared Error - <code>r2</code>: R\u00b2 Score - <code>mape</code>: Mean Absolute Percentage Error - <code>iou</code>: Intersection over Union</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#detection-task","title":"Detection Task","text":"<pre><code># Object detection example\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nresults = model.predict(source='images/', verbose=False)\n\nmap_value, ci = evaluator.evaluate(\n    y_true='dataset_path',  # Path to dataset or ground truth\n    y_pred=results,\n    task='detection',\n    metric='map',\n    method='bootstrap_percentile',\n    n_resamples=1000\n)\n</code></pre> <p>Available metrics: - <code>map</code>: mean Average Precision @0.5:0.95 - <code>map50</code>: mean Average Precision @0.5 - <code>precision</code>: Detection precision - <code>recall</code>: Detection recall</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#available-ci-methods","title":"Available CI Methods","text":"<p>Check which methods are available for each task:</p> <pre><code># Classification methods\nmethods = evaluator.get_available_methods('classification')\nprint(methods)\n# ['wilson', 'normal', 'agresti_coull', 'beta', 'jeffreys', 'binom_test',\n#  'bootstrap_bca', 'bootstrap_percentile', 'jackknife']\n\n# Regression methods\nmethods = evaluator.get_available_methods('regression')\nprint(methods)\n# ['bootstrap_bca', 'bootstrap_percentile', 'jackknife']\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#advanced-usage","title":"Advanced Usage","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#comparing-different-ci-methods","title":"Comparing Different CI Methods","text":"<pre><code>import numpy as np\n\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n\nevaluator = MetricEvaluator()\n\n# Compare different methods\nmethods = ['wilson', 'normal', 'agresti_coull', 'bootstrap_bca']\n\nfor method in methods:\n    acc, ci = evaluator.evaluate(\n        y_true=y_true,\n        y_pred=y_pred,\n        task='classification',\n        metric='accuracy',\n        method=method,\n        n_resamples=2000  # Only used by bootstrap\n    )\n    width = ci[1] - ci[0]\n    print(f\"{method:20s}: {acc:.3f} [{ci[0]:.3f}, {ci[1]:.3f}] (width: {width:.3f})\")\n</code></pre> <p>Output: <pre><code>wilson              : 0.800 [0.519, 0.938] (width: 0.419)\nnormal              : 0.800 [0.596, 1.004] (width: 0.408)\nagresti_coull       : 0.800 [0.529, 0.933] (width: 0.404)\nbootstrap_bca       : 0.800 [0.533, 0.933] (width: 0.400)\n</code></pre></p>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#regression-with-visualization","title":"Regression with Visualization","text":"<pre><code>from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Generate data\nX, y = make_regression(n_samples=100, n_features=5, noise=10.0, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Evaluate with visualization\nevaluator = MetricEvaluator()\nr2_value, ci = evaluator.evaluate(\n    y_true=y_test,\n    y_pred=y_pred,\n    task='regression',\n    metric='r2',\n    method='bootstrap_bca',\n    n_resamples=2000,\n    plot=True  # Generate histogram of bootstrap samples\n)\n\nprint(f\"R\u00b2 Score: {r2_value:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#multi-class-classification-with-macromicro-averaging","title":"Multi-class Classification with Macro/Micro Averaging","text":"<pre><code>y_true = [0, 1, 2, 2, 2, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1, 1]\ny_pred = [0, 1, 0, 0, 2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 1, 2, 2, 1, 1]\n\nevaluator = MetricEvaluator()\n\n# Macro F1 (average across classes)\nmacro_f1, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='f1',\n    method='takahashi',  # Analytical method for multi-class\n    average='macro'\n)\n\nprint(f\"Macro F1: {macro_f1:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n\n# Micro F1 (aggregate contributions of all classes)\nmicro_f1, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='f1',\n    method='takahashi',\n    average='micro'\n)\n\nprint(f\"Micro F1: {micro_f1:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#parameters","title":"Parameters","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#evaluate-method","title":"evaluate() method","text":"<p>Parameters:</p> <ul> <li>y_true (array-like): Ground truth labels/values</li> <li>y_pred (array-like): Predicted labels/values</li> <li>task (str): Type of task - 'classification', 'regression', or 'detection'</li> <li>metric (str): Name of the metric to compute</li> <li>method (str, optional): CI calculation method (default: 'bootstrap_bca')</li> <li>confidence_interval (float, optional): Confidence level (default: 0.95)</li> <li>n_resamples (int, optional): Bootstrap resamples (default: 2000)</li> <li>random_state (int, optional): Random seed for reproducibility</li> <li>plot (bool, optional): Generate visualization (default: False)</li> <li>**kwargs: Additional metric-specific parameters</li> </ul> <p>Returns:</p> <ul> <li>metric_value (float): The computed metric value</li> <li>ci (tuple): Confidence interval (lower_bound, upper_bound)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#method-selection-guide","title":"Method Selection Guide","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#when-to-use-analytical-methods-classification","title":"When to use analytical methods (Classification)","text":"<ul> <li>wilson: Recommended for most binary classification metrics (accuracy, precision, recall)</li> <li>normal: Simple but may give intervals outside [0,1] for small samples</li> <li>agresti_coull: Good alternative to Wilson</li> <li>beta/jeffreys: Conservative intervals, good for small samples</li> <li>binom_test: Exact method, computationally intensive</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#when-to-use-bootstrap-methods","title":"When to use bootstrap methods","text":"<ul> <li>bootstrap_bca: Recommended default, corrects for bias and skewness</li> <li>bootstrap_percentile: Simpler alternative, good for symmetric distributions</li> <li>bootstrap_basic: Basic method, less accurate than BCA</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#when-to-use-jackknife","title":"When to use jackknife","text":"<ul> <li>jackknife: Good for small samples, less computationally intensive than bootstrap</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#when-to-use-takahashi-methods","title":"When to use Takahashi methods","text":"<ul> <li>takahashi: Specifically for multi-class F1, precision, recall with macro/micro averaging</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#error-handling","title":"Error Handling","text":"<pre><code># Invalid task\ntry:\n    evaluator.evaluate(y_true, y_pred, task='invalid', metric='accuracy')\nexcept ValueError as e:\n    print(e)  # \"Task must be one of: classification, regression, detection\"\n\n# Invalid metric for task\ntry:\n    evaluator.evaluate(y_true, y_pred, task='regression', metric='accuracy')\nexcept ValueError as e:\n    print(e)  # \"Metric 'accuracy' not available for regression task\"\n\n# Invalid method\ntry:\n    evaluator.evaluate(y_true, y_pred, task='regression', metric='mae', method='wilson')\nexcept ValueError as e:\n    print(e)  # \"Method 'wilson' not available for regression task\"\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/MetricEvaluator/#see-also","title":"See Also","text":"<ul> <li>Classification Metrics</li> <li>Regression Metrics</li> <li>Detection Metrics</li> <li>CI Methods Guide</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/","title":"accuracy_score","text":"<p>Compute classification accuracy with confidence intervals.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#infer_ci.binary_metrics.accuracy_score","title":"<code>infer_ci.binary_metrics.accuracy_score(y_true, y_pred, confidence_level=0.95, method='wilson', compute_ci=True, **kwargs)</code>","text":"<p>Compute the accuracy score and optionally the confidence interval. Parameters</p> <p>y_true : List[int]     The grount truth labels. y_pred : List[int]     The predicted categories. confidence_level : float, optional     The confidence interval level, by default 0.95 method : str, optional     The method for the stats model proportion method, by default 'wilson' compute_ci : bool, optional     If true return the confidence interval as well as the accuract score, by default True</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#infer_ci.binary_metrics.accuracy_score--returns","title":"Returns","text":"<p>Union[float, Tuple[float, Tuple[float, float]]]     The accuracy score and optionally the confidence interval.</p> Source code in <code>src/infer_ci/binary_metrics.py</code> <pre><code>def accuracy_score(y_true: List[int],\n                   y_pred: List[int],\n                   confidence_level: float = 0.95,\n                   method: str = 'wilson',\n                   compute_ci: bool = True,\n                   **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]]]:\n    \"\"\"\n        Compute the accuracy score and optionally the confidence interval.\n        Parameters\n        ----------\n        y_true : List[int]\n            The grount truth labels.\n        y_pred : List[int]\n            The predicted categories.\n        confidence_level : float, optional\n            The confidence interval level, by default 0.95\n        method : str, optional\n            The method for the stats model proportion method, by default 'wilson'\n        compute_ci : bool, optional\n            If true return the confidence interval as well as the accuract score, by default True\n\n        Returns\n        -------\n        Union[float, Tuple[float, Tuple[float, float]]]\n            The accuracy score and optionally the confidence interval.\n    \"\"\"\n    if method in bootstrap_methods:\n        return accuracy_score_bootstrap(\n            y_true, y_pred, confidence_level, method, **kwargs)\n    else:\n        return accuracy_score_binomial_ci(\n            y_true, y_pred, confidence_level, method, compute_ci)\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#overview","title":"Overview","text":"<p>Accuracy is the fraction of predictions that match the true labels. It's the most intuitive performance measure for classification but can be misleading for imbalanced datasets.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#mathematical-definition","title":"Mathematical Definition","text":"\\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\] <p>Where: - TP = True Positives - TN = True Negatives - FP = False Positives - FN = False Negatives</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#examples","title":"Examples","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#basic-usage","title":"Basic Usage","text":"<pre><code>from infer_ci import accuracy_score\nimport numpy as np\n\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1])\n\n# Using Wilson score interval (recommended)\nacc, ci = accuracy_score(y_true, y_pred, method='wilson')\n\nprint(f\"Accuracy: {acc:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>Accuracy: 0.900\n95% CI: [0.597, 0.983]\n</code></pre></p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#comparing-different-ci-methods","title":"Comparing Different CI Methods","text":"<pre><code>import numpy as np\nfrom infer_ci import accuracy_score\n\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n\nmethods = ['wilson', 'normal', 'agresti_coull', 'bootstrap_bca']\n\nfor method in methods:\n    acc, ci = accuracy_score(y_true, y_pred, method=method, n_resamples=2000)\n    print(f\"{method:20s}: {acc:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>wilson              : 0.800 [0.519, 0.938]\nnormal              : 0.800 [0.596, 1.004]\nagresti_coull       : 0.800 [0.529, 0.933]\nbootstrap_bca       : 0.800 [0.533, 0.933]\n</code></pre></p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#with-metricevaluator","title":"With MetricEvaluator","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\nevaluator = MetricEvaluator()\n\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 1, 0, 0])\n\nacc, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='accuracy',\n    method='wilson'\n)\n\nprint(f\"Accuracy: {acc:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#without-computing-ci","title":"Without Computing CI","text":"<pre><code>from infer_ci import accuracy_score\nimport numpy as np\n\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 1, 0, 0])\n\n# Just get the point estimate\nacc, _ = accuracy_score(y_true, y_pred, compute_ci=False)\n\nprint(f\"Accuracy: {acc:.3f}\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#parameters","title":"Parameters","text":"<ul> <li>y_true (array-like): Ground truth binary labels (0 or 1)</li> <li>y_pred (array-like): Predicted binary labels (0 or 1)</li> <li>method (str, optional): CI calculation method. Default: 'bootstrap_bca'</li> <li>Analytical: 'wilson', 'normal', 'agresti_coull', 'beta', 'jeffreys', 'binom_test'</li> <li>Bootstrap: 'bootstrap_bca', 'bootstrap_percentile', 'bootstrap_basic'</li> <li>Other: 'jackknife'</li> <li>confidence_interval (float, optional): Confidence level (0 &lt; CI &lt; 1). Default: 0.95</li> <li>n_resamples (int, optional): Number of bootstrap resamples. Default: 2000</li> <li>compute_ci (bool, optional): Whether to compute CI. Default: True</li> <li>random_state (int, optional): Random seed for reproducibility</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#returns","title":"Returns","text":"<ul> <li>accuracy (float): The computed accuracy value (between 0 and 1)</li> <li>ci (tuple): Confidence interval (lower_bound, upper_bound)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#notes","title":"Notes","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#when-to-use-accuracy","title":"When to Use Accuracy","text":"<p>\u2705 Good for: - Balanced datasets - Binary or multi-class problems where all classes are equally important - Quick model comparison</p> <p>\u274c Not recommended for: - Imbalanced datasets (use F1, precision, recall instead) - Medical diagnosis (prefer sensitivity/specificity) - When false positives and false negatives have different costs</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#method-recommendations","title":"Method Recommendations","text":"<p>For accuracy with binary classification:</p> <ol> <li>Wilson score (recommended): Best coverage properties, handles edge cases well</li> <li>Agresti-Coull: Good alternative, similar performance to Wilson</li> <li>Bootstrap BCA: Use when you want a non-parametric approach</li> <li>Normal approximation: Simple but may give intervals outside [0, 1]</li> </ol>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#edge-cases","title":"Edge Cases","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#perfect-accuracy","title":"Perfect Accuracy","text":"<pre><code>from infer_ci import accuracy_score\nimport numpy as np\n\ny_true = np.array([1, 1, 1, 1, 1])\ny_pred = np.array([1, 1, 1, 1, 1])\n\nacc, ci = accuracy_score(y_true, y_pred, method='wilson')\n\nprint(f\"Accuracy: {acc:.3f}\")  # 1.000\nprint(f\"CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")  # [0.566, 1.000]\n</code></pre> <p>Note: Even with perfect accuracy, the CI lower bound is not 1.0 due to sampling uncertainty.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#small-sample-size","title":"Small Sample Size","text":"<pre><code>from infer_ci import accuracy_score\nimport numpy as np\n\n# Very small sample\ny_true = np.array([0, 1, 1])\ny_pred = np.array([0, 1, 1])\n\nacc, ci = accuracy_score(y_true, y_pred, method='wilson')\n\nprint(f\"Accuracy: {acc:.3f}\")  # 1.000\nprint(f\"CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")  # Very wide interval\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#see-also","title":"See Also","text":"<ul> <li>precision_score - Positive Predictive Value</li> <li>recall_score - True Positive Rate</li> <li>f1_score - Harmonic mean of precision and recall</li> <li>MetricEvaluator - Unified evaluation interface</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/classification/accuracy_score/#references","title":"References","text":"<ul> <li>Wilson, E. B. (1927). \"Probable inference, the law of succession, and statistical inference\". Journal of the American Statistical Association.</li> <li>Agresti, A., &amp; Coull, B. A. (1998). \"Approximate is better than 'exact' for interval estimation of binomial proportions\". The American Statistician.</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/","title":"mae","text":"<p>Compute Mean Absolute Error (MAE) with confidence intervals.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#infer_ci.regression_metrics.mae","title":"<code>infer_ci.regression_metrics.mae(y_true, y_pred, confidence_level=0.95, method='bootstrap_bca', compute_ci=True, plot=False, **kwargs)</code>","text":"<p>Compute the Mean Absolute Error and optionally the confidence interval.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#infer_ci.regression_metrics.mae--parameters","title":"Parameters","text":"<p>y_true : List[float]     The ground truth target values. y_pred : List[float]     The predicted target values. confidence_level : float, optional     The confidence interval level, by default 0.95 method : str, optional     The bootstrap method, by default 'bootstrap_bca' compute_ci : bool, optional     If true return the confidence interval as well as the MAE score, by default True plot : bool, optional     If true create histogram plot for bootstrap methods, by default False</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#infer_ci.regression_metrics.mae--returns","title":"Returns","text":"<p>Union[float, Tuple[float, Tuple[float, float]]]     The MAE score and optionally the confidence interval.</p> Source code in <code>src/infer_ci/regression_metrics.py</code> <pre><code>def mae(y_true: List[float],\n        y_pred: List[float],\n        confidence_level: float = 0.95,\n        method: str = 'bootstrap_bca',\n        compute_ci: bool = True,\n        plot: bool = False,\n        **kwargs) -&gt; Union[float, Tuple[float, Tuple[float, float]]]:\n    \"\"\"\n    Compute the Mean Absolute Error and optionally the confidence interval.\n\n    Parameters\n    ----------\n    y_true : List[float]\n        The ground truth target values.\n    y_pred : List[float]\n        The predicted target values.\n    confidence_level : float, optional\n        The confidence interval level, by default 0.95\n    method : str, optional\n        The bootstrap method, by default 'bootstrap_bca'\n    compute_ci : bool, optional\n        If true return the confidence interval as well as the MAE score, by default True\n    plot : bool, optional\n        If true create histogram plot for bootstrap methods, by default False\n\n    Returns\n    -------\n    Union[float, Tuple[float, Tuple[float, float]]]\n        The MAE score and optionally the confidence interval.\n    \"\"\"\n    def mae_metric(y_true, y_pred):\n        return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n\n    return _compute_metric_with_optional_ci(\n        y_true=y_true,\n        y_pred=y_pred,\n        metric_func=mae_metric,\n        confidence_level=confidence_level,\n        method=method,\n        compute_ci=compute_ci,\n        plot=plot,\n        metric_name=\"mae\",\n        **kwargs\n    )\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#overview","title":"Overview","text":"<p>Mean Absolute Error (MAE) measures the average magnitude of errors in a set of predictions, without considering their direction. It's the average of the absolute differences between predictions and actual observations.</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#mathematical-definition","title":"Mathematical Definition","text":"\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\] <p>Where: - \\(y_i\\) = actual value - \\(\\hat{y}_i\\) = predicted value - \\(n\\) = number of samples</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#examples","title":"Examples","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#basic-usage","title":"Basic Usage","text":"<pre><code>from infer_ci import mae\nimport numpy as np\n\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])\n\n# Using bootstrap BCA method (default)\nmae_value, ci = mae(y_true, y_pred)\n\nprint(f\"MAE: {mae_value:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>MAE: 0.500\n95% CI: [0.125, 0.875]\n</code></pre></p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#comparing-ci-methods","title":"Comparing CI Methods","text":"<pre><code>from infer_ci import mae\nimport numpy as np\n\ny_true = np.array([3.0, -0.5, 2.0, 7.0, 5.0, 1.0, 4.0, 3.5])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0, 4.5, 1.5, 3.8, 3.2])\n\nmethods = ['bootstrap_bca', 'bootstrap_percentile', 'jackknife']\n\nfor method in methods:\n    mae_val, ci = mae(y_true, y_pred, method=method, n_resamples=2000)\n    print(f\"{method:22s}: {mae_val:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>bootstrap_bca         : 0.438 [0.200, 0.688]\nbootstrap_percentile  : 0.438 [0.219, 0.700]\njackknife             : 0.438 [0.182, 0.693]\n</code></pre></p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#with-metricevaluator","title":"With MetricEvaluator","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\nevaluator = MetricEvaluator()\n\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])\n\nmae_val, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='regression',\n    metric='mae',\n    method='bootstrap_bca',\n    n_resamples=2000\n)\n\nprint(f\"MAE: {mae_val:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#real-world-example-linear-regression","title":"Real-World Example: Linear Regression","text":"<pre><code>from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom infer_ci import mae\nimport numpy as np\n\n# Generate synthetic data\nX, y = make_regression(n_samples=100, n_features=5, noise=10.0, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Evaluate with confidence interval\nmae_val, ci = mae(y_test, y_pred, method='bootstrap_bca', n_resamples=2000, random_state=42)\n\nprint(f\"Mean Absolute Error: {mae_val:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\nprint(f\"CI Width: {ci[1] - ci[0]:.3f}\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#with-visualization","title":"With Visualization","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\ny_true = np.random.normal(10, 2, 100)\ny_pred = y_true + np.random.normal(0, 1, 100)\n\nevaluator = MetricEvaluator()\n\nmae_val, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='regression',\n    metric='mae',\n    method='bootstrap_bca',\n    n_resamples=2000,\n    plot=True  # Generate histogram of bootstrap distribution\n)\n\nprint(f\"MAE: {mae_val:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#parameters","title":"Parameters","text":"<ul> <li>y_true (array-like): Ground truth continuous values</li> <li>y_pred (array-like): Predicted continuous values</li> <li>method (str, optional): CI calculation method. Default: 'bootstrap_bca'</li> <li>Bootstrap: 'bootstrap_bca', 'bootstrap_percentile', 'bootstrap_basic'</li> <li>Other: 'jackknife'</li> <li>confidence_interval (float, optional): Confidence level (0 &lt; CI &lt; 1). Default: 0.95</li> <li>n_resamples (int, optional): Number of bootstrap resamples. Default: 2000</li> <li>compute_ci (bool, optional): Whether to compute CI. Default: True</li> <li>random_state (int, optional): Random seed for reproducibility</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#returns","title":"Returns","text":"<ul> <li>mae_value (float): The computed MAE value (\u2265 0)</li> <li>ci (tuple): Confidence interval (lower_bound, upper_bound)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#interpretation","title":"Interpretation","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#scale","title":"Scale","text":"<ul> <li>Lower is better: MAE = 0 indicates perfect predictions</li> <li>Same units as target: MAE is in the same units as the target variable, making it easy to interpret</li> <li>Range: [0, \u221e)</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#comparing-models","title":"Comparing Models","text":"<pre><code>from infer_ci import mae\nimport numpy as np\n\ny_true = np.array([3.0, -0.5, 2.0, 7.0, 5.0, 1.0, 4.0, 3.5])\n\n# Model 1: Simple predictor\ny_pred1 = np.array([2.5, 0.0, 2.0, 8.0, 4.5, 1.5, 3.8, 3.2])\n\n# Model 2: Better predictor\ny_pred2 = np.array([3.1, -0.4, 2.1, 7.1, 4.9, 1.1, 4.1, 3.4])\n\nmae1, ci1 = mae(y_true, y_pred1, random_state=42)\nmae2, ci2 = mae(y_true, y_pred2, random_state=42)\n\nprint(f\"Model 1 MAE: {mae1:.3f} [{ci1[0]:.3f}, {ci1[1]:.3f}]\")\nprint(f\"Model 2 MAE: {mae2:.3f} [{ci2[0]:.3f}, {ci2[1]:.3f}]\")\n\n# Check if CIs overlap\nif ci2[1] &lt; ci1[0]:\n    print(\"Model 2 is significantly better!\")\nelif ci1[1] &lt; ci2[0]:\n    print(\"Model 1 is significantly better!\")\nelse:\n    print(\"No significant difference between models\")\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#advantages","title":"Advantages","text":"<p>\u2705 Pros: - Easy to understand and interpret - Same units as the target variable - Less sensitive to outliers than MSE - All errors weighted equally</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#disadvantages","title":"Disadvantages","text":"<p>\u274c Cons: - Doesn't indicate direction of errors - Not differentiable at zero (optimization issues) - Doesn't penalize large errors as heavily as MSE</p>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#when-to-use-mae","title":"When to Use MAE","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#use-mae-when","title":"Use MAE when:","text":"<ul> <li>You want errors in original units</li> <li>Outliers should not dominate the metric</li> <li>All error magnitudes are equally important</li> <li>Interpretability is crucial</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#consider-alternatives-when","title":"Consider alternatives when:","text":"<ul> <li>Large errors should be penalized more \u2192 use MSE or RMSE</li> <li>You need relative errors \u2192 use MAPE</li> <li>You want to explain variance \u2192 use R\u00b2</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#comparison-with-other-metrics","title":"Comparison with Other Metrics","text":"Metric Unit Outlier Sensitivity Differentiable MAE Original Low No MSE Squared High Yes RMSE Original High Yes MAPE Percentage Medium No","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#edge-cases","title":"Edge Cases","text":"","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#perfect-predictions","title":"Perfect Predictions","text":"<pre><code>from infer_ci import mae\nimport numpy as np\n\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 5])\n\nmae_val, ci = mae(y_true, y_pred)\n\nprint(f\"MAE: {mae_val:.3f}\")  # 0.000\nprint(f\"CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")  # [0.000, 0.000]\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#single-sample","title":"Single Sample","text":"<pre><code>from infer_ci import mae\nimport numpy as np\n\ny_true = np.array([5.0])\ny_pred = np.array([4.0])\n\nmae_val, ci = mae(y_true, y_pred, method='bootstrap_bca')\n\nprint(f\"MAE: {mae_val:.3f}\")  # 1.000\n# Note: CI will be very wide due to single sample\n</code></pre>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#see-also","title":"See Also","text":"<ul> <li>mse - Mean Squared Error</li> <li>rmse - Root Mean Squared Error</li> <li>mape - Mean Absolute Percentage Error</li> <li>r2_score - Coefficient of Determination</li> <li>MetricEvaluator - Unified evaluation interface</li> </ul>","tags":["api-docs","api-reference"]},{"location":"api-docs/regression/mae/#references","title":"References","text":"<ul> <li>Willmott, C. J., &amp; Matsuura, K. (2005). \"Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance\". Climate Research.</li> <li>Chai, T., &amp; Draxler, R. R. (2014). \"Root mean square error (RMSE) or mean absolute error (MAE)?\". Geoscientific Model Development.</li> </ul>","tags":["api-docs","api-reference"]},{"location":"dev/build/","title":"\ud83c\udfd7\ufe0f Build Python Package","text":"<p>To build the python package, run the following command:</p> <pre><code># Install python build dependencies:\npip install -r ./requirements/requirements.build.txt\n\n# Build python package:\npython -m build\n# Or use the build script:\n./scripts/build.sh\n</code></pre>","tags":["dev","development"]},{"location":"dev/build/#build","title":"Build","text":"<pre><code># Install python build:\npip install -U build\n\n# Build help:\npython -m build --help\n</code></pre>","tags":["dev","development"]},{"location":"dev/build/#references","title":"References","text":"<ul> <li>Python Packaging User Guide</li> <li>Packaging Python Projects</li> <li>Writing your <code>pyproject.toml</code></li> <li>Setuptools Documentation</li> <li>Blogs:<ul> <li>Python Packaging Best Practices</li> <li>Generic Folder Structure for your Machine Learning Projects</li> <li>How to Upload your Python Package to PyPI</li> </ul> </li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/","title":"\ud83e\udd1d Contributing","text":"<p>Thank you for your interest in contributing to infer-ci! This guide will help you get started with contributing to the project.</p>","tags":["dev","development"]},{"location":"dev/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>Running Tests</li> <li>Code Style</li> <li>Adding New Metrics</li> <li>Documentation</li> <li>Pull Request Process</li> <li>Commit Message Guidelines</li> <li>Testing Requirements</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a code of conduct that all contributors are expected to follow. Please be respectful and constructive in all interactions.</p>","tags":["dev","development"]},{"location":"dev/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/infer-ci.git\ncd infer-ci\n</code></pre></li> <li>Add upstream remote:    <pre><code>git remote add upstream https://github.com/humblebeeai/infer-ci.git\n</code></pre></li> </ol>","tags":["dev","development"]},{"location":"dev/contributing/#development-setup","title":"Development Setup","text":"","tags":["dev","development"]},{"location":"dev/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> <li>Git</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#installation","title":"Installation","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol> <p>This installs the package in editable mode with all development dependencies including:    - pytest (testing framework)    - pytest-cov (coverage reporting)    - black (code formatting)    - flake8 (linting)    - mypy (type checking)    - mkdocs and plugins (documentation)</p> <ol> <li>Install pre-commit hooks (optional but recommended):    <pre><code>pre-commit install\n</code></pre></li> </ol>","tags":["dev","development"]},{"location":"dev/contributing/#running-tests","title":"Running Tests","text":"","tags":["dev","development"]},{"location":"dev/contributing/#run-all-tests","title":"Run all tests","text":"<pre><code>pytest\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#run-tests-with-coverage","title":"Run tests with coverage","text":"<pre><code>pytest --cov=infer_ci --cov-report=html\n</code></pre> <p>View the coverage report by opening <code>htmlcov/index.html</code> in your browser.</p>","tags":["dev","development"]},{"location":"dev/contributing/#run-specific-test-files","title":"Run specific test files","text":"<pre><code>pytest tests/test_classification.py\npytest tests/test_regression.py\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#run-tests-with-verbose-output","title":"Run tests with verbose output","text":"<pre><code>pytest -v\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#using-the-test-script","title":"Using the test script","text":"<p>The project includes a convenience script for running tests:</p> <pre><code>./scripts/test.sh        # Run all tests\n./scripts/test.sh -c     # Run with coverage\n./scripts/test.sh -l     # Run linting checks\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#code-style","title":"Code Style","text":"<p>This project follows strict code quality standards:</p>","tags":["dev","development"]},{"location":"dev/contributing/#formatting","title":"Formatting","text":"<p>We use Black for code formatting with a line length of 100:</p> <pre><code>black src/infer_ci tests\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#linting","title":"Linting","text":"<p>We use flake8 for linting:</p> <pre><code>flake8 src/infer_ci tests\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#type-checking","title":"Type Checking","text":"<p>We use mypy for static type checking:</p> <pre><code>mypy src/infer_ci\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks automatically check code quality before each commit. If you installed them, they will run automatically. To run manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#adding-new-metrics","title":"Adding New Metrics","text":"<p>Adding a new metric involves several steps. Here's a complete guide:</p>","tags":["dev","development"]},{"location":"dev/contributing/#1-determine-the-metric-type","title":"1. Determine the Metric Type","text":"<p>First, identify which category your metric belongs to: - Classification: Metrics for classification tasks (accuracy, precision, recall, F1, etc.) - Regression: Metrics for regression tasks (MAE, MSE, RMSE, R\u00b2, etc.) - Detection: Metrics for object detection (mAP, precision, recall)</p>","tags":["dev","development"]},{"location":"dev/contributing/#2-implement-the-metric-function","title":"2. Implement the Metric Function","text":"<p>Create your metric function in the appropriate module:</p> <p>For classification metrics: Add to <code>src/infer_ci/binary_metrics.py</code> or <code>src/infer_ci/multiclass_metrics.py</code></p> <p>For regression metrics: Add to <code>src/infer_ci/regression_metrics.py</code></p> <p>Example template:</p> <pre><code>from typing import Tuple, Optional\nimport numpy as np\nfrom .methods import compute_ci\n\ndef my_new_metric(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    method: str = 'bootstrap_bca',\n    confidence_interval: float = 0.95,\n    n_resamples: int = 2000,\n    compute_ci: bool = True,\n    random_state: Optional[int] = None,\n    **kwargs\n) -&gt; Tuple[float, Tuple[float, float]]:\n    \"\"\"\n    Compute my new metric with confidence interval.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground truth labels\n    y_pred : np.ndarray\n        Predicted labels\n    method : str, default='bootstrap_bca'\n        Method for computing confidence interval\n    confidence_interval : float, default=0.95\n        Confidence level (0 &lt; CI &lt; 1)\n    n_resamples : int, default=2000\n        Number of bootstrap resamples\n    compute_ci : bool, default=True\n        Whether to compute confidence interval\n    random_state : int, optional\n        Random seed for reproducibility\n\n    Returns\n    -------\n    metric_value : float\n        The computed metric value\n    ci : Tuple[float, float]\n        Confidence interval (lower, upper)\n\n    Examples\n    --------\n    &gt;&gt;&gt; y_true = np.array([0, 1, 1, 0, 1])\n    &gt;&gt;&gt; y_pred = np.array([0, 1, 1, 0, 0])\n    &gt;&gt;&gt; value, ci = my_new_metric(y_true, y_pred)\n    &gt;&gt;&gt; print(f\"Metric: {value:.3f}, CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n    \"\"\"\n    # Your metric computation logic here\n    def metric_func(y_true, y_pred):\n        # Calculate your metric\n        return result\n\n    metric_value = metric_func(y_true, y_pred)\n\n    if not compute_ci:\n        return metric_value, (np.nan, np.nan)\n\n    ci = compute_ci(\n        y_true=y_true,\n        y_pred=y_pred,\n        metric_func=metric_func,\n        method=method,\n        confidence_interval=confidence_interval,\n        n_resamples=n_resamples,\n        random_state=random_state,\n        **kwargs\n    )\n\n    return metric_value, ci\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#3-register-the-metric-in-metricevaluator","title":"3. Register the Metric in MetricEvaluator","text":"<p>Add your metric to <code>src/infer_ci/evaluator.py</code>:</p> <pre><code># In the appropriate task section (classification/regression/detection)\nCLASSIFICATION_METRICS = {\n    # ... existing metrics ...\n    'my_new_metric': my_new_metric,\n}\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#4-export-the-metric","title":"4. Export the Metric","text":"<p>Add your metric to <code>src/infer_ci/__init__.py</code>:</p> <pre><code>from .binary_metrics import (\n    # ... existing imports ...\n    my_new_metric,\n)\n\n__all__ = [\n    # ... existing exports ...\n    'my_new_metric',\n]\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#5-write-tests","title":"5. Write Tests","text":"<p>Create comprehensive tests in the appropriate test file:</p> <pre><code># tests/test_my_metric.py\nimport numpy as np\nimport pytest\nfrom infer_ci import my_new_metric, MetricEvaluator\n\ndef test_my_new_metric_basic():\n    \"\"\"Test basic functionality.\"\"\"\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0, 0])\n\n    value, ci = my_new_metric(y_true, y_pred)\n\n    assert isinstance(value, float)\n    assert len(ci) == 2\n    assert ci[0] &lt;= value &lt;= ci[1]\n\ndef test_my_new_metric_evaluator():\n    \"\"\"Test through MetricEvaluator interface.\"\"\"\n    evaluator = MetricEvaluator()\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0, 0])\n\n    value, ci = evaluator.evaluate(\n        y_true=y_true,\n        y_pred=y_pred,\n        task='classification',\n        metric='my_new_metric',\n        method='wilson'\n    )\n\n    assert value &gt; 0\n\ndef test_my_new_metric_edge_cases():\n    \"\"\"Test edge cases.\"\"\"\n    # Perfect predictions\n    y_true = np.array([1, 1, 1, 1])\n    y_pred = np.array([1, 1, 1, 1])\n    value, _ = my_new_metric(y_true, y_pred, compute_ci=False)\n    assert value == 1.0\n\n    # All wrong\n    y_true = np.array([0, 0, 0, 0])\n    y_pred = np.array([1, 1, 1, 1])\n    value, _ = my_new_metric(y_true, y_pred, compute_ci=False)\n    assert value == 0.0\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#6-add-documentation","title":"6. Add Documentation","text":"<p>Create API documentation in <code>docs/api-docs/</code>:</p> <pre><code># my_new_metric\n\n::: infer_ci.binary_metrics.my_new_metric\n    options:\n      show_root_heading: true\n      show_source: true\n\n## Examples\n\n[Your examples here]\n\n## Mathematical Definition\n\n[Your formula here]\n\n## References\n\n[Your citations here]\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#7-update-changelogmd","title":"7. Update CHANGELOG.md","text":"<p>Add an entry to the <code>[Unreleased]</code> section:</p> <pre><code>### Added\n- New metric: `my_new_metric` for [task type] with confidence intervals\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#documentation","title":"Documentation","text":"","tags":["dev","development"]},{"location":"dev/contributing/#building-documentation-locally","title":"Building Documentation Locally","text":"<pre><code>mkdocs serve\n</code></pre> <p>Then visit <code>http://127.0.0.1:8000</code> in your browser.</p>","tags":["dev","development"]},{"location":"dev/contributing/#documentation-structure","title":"Documentation Structure","text":"<ul> <li><code>docs/getting-started/</code> - Tutorials and quick starts</li> <li><code>docs/api-docs/</code> - API reference documentation</li> <li><code>docs/dev/</code> - Developer documentation</li> <li><code>README.md</code> - Main package documentation</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Explain mathematical concepts when relevant</li> <li>Add references to papers/sources</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch from <code>main</code>:    <pre><code>git checkout -b feature/my-new-feature\n</code></pre></p> </li> <li> <p>Make your changes following the code style guidelines</p> </li> <li> <p>Write or update tests to maintain coverage above 80%</p> </li> <li> <p>Update documentation as needed</p> </li> <li> <p>Run tests and checks:    <pre><code>pytest --cov=infer_ci\nblack src/infer_ci tests\nflake8 src/infer_ci tests\n</code></pre></p> </li> <li> <p>Commit your changes following the commit message guidelines</p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/my-new-feature\n</code></pre></p> </li> <li> <p>Open a Pull Request on GitHub with:</p> </li> <li>Clear title describing the change</li> <li>Description of what changed and why</li> <li>Reference to any related issues</li> <li> <p>Screenshots/examples if applicable</p> </li> <li> <p>Address review feedback if requested</p> </li> <li> <p>Wait for CI checks to pass before merging</p> </li> </ol>","tags":["dev","development"]},{"location":"dev/contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>We follow conventional commit format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#types","title":"Types","text":"<ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>style</code>: Code style changes (formatting, etc.)</li> <li><code>refactor</code>: Code refactoring</li> <li><code>test</code>: Adding or updating tests</li> <li><code>chore</code>: Maintenance tasks</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#examples","title":"Examples","text":"<pre><code>feat(metrics): add IoU metric for regression tasks\n\nImplemented Intersection over Union metric with bootstrap\nconfidence intervals for regression evaluation.\n\nCloses #123\n</code></pre> <pre><code>fix(bootstrap): correct BCa interval calculation\n\nFixed bias correction in bootstrap BCa method that was\ncausing incorrect confidence intervals for small samples.\n</code></pre> <pre><code>docs(api): add examples for F1 score usage\n\nAdded comprehensive examples showing binary, macro, and\nmicro averaging for F1 score computation.\n</code></pre>","tags":["dev","development"]},{"location":"dev/contributing/#testing-requirements","title":"Testing Requirements","text":"","tags":["dev","development"]},{"location":"dev/contributing/#coverage-standards","title":"Coverage Standards","text":"<ul> <li>Minimum coverage: 80% overall</li> <li>New code: Should have at least 90% coverage</li> <li>Critical paths: Must have 100% coverage</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#what-to-test","title":"What to Test","text":"<ol> <li>Basic functionality: Does it work as expected?</li> <li>Edge cases: Empty arrays, perfect scores, all zeros</li> <li>Invalid inputs: Wrong shapes, invalid parameters</li> <li>Integration: Works through MetricEvaluator</li> <li>Reproducibility: Random state produces consistent results</li> <li>CI methods: Different methods produce valid intervals</li> </ol>","tags":["dev","development"]},{"location":"dev/contributing/#test-organization","title":"Test Organization","text":"<ul> <li><code>tests/test_classification.py</code> - Classification metrics</li> <li><code>tests/test_regression.py</code> - Regression metrics</li> <li><code>tests/test_detection.py</code> - Detection metrics</li> <li><code>tests/test_methods.py</code> - CI calculation methods</li> <li><code>tests/test_evaluator.py</code> - MetricEvaluator interface</li> </ul>","tags":["dev","development"]},{"location":"dev/contributing/#questions","title":"Questions?","text":"<p>If you have questions:</p> <ol> <li>Check the documentation</li> <li>Search existing issues</li> <li>Open a new issue</li> </ol>","tags":["dev","development"]},{"location":"dev/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>","tags":["dev","development"]},{"location":"dev/diagrams/","title":"\ud83d\uddbc\ufe0f Diagrams","text":"<p>This page contains diagrams that illustrate the architecture of the module.</p>","tags":["dev","development"]},{"location":"dev/diagrams/#class","title":"Class","text":"","tags":["dev","development"]},{"location":"dev/diagrams/#package","title":"Package","text":"","tags":["dev","development"]},{"location":"dev/docs/","title":"\ud83d\udcdd Docs","text":"<p>To build the documentation, run the following command:</p> <pre><code># Install python documentation dependencies:\npip install -r ./requirements/requirements.docs.txt\n\n# Serve documentation locally (for development):\nmkdocs serve\n# Or use the docs script:\n./scripts/docs.sh\n\n# Or build documentation:\nmkdocs build\n# Or use the docs script:\n./scripts/docs.sh -b\n</code></pre>","tags":["dev","development"]},{"location":"dev/docs/#diagrams","title":"Diagrams","text":"<p>Prerequisites:</p> <ul> <li>Install Graphviz</li> </ul> <p>To generate diagrams, run the following command:</p> <pre><code># Install python documentation dependencies:\npip install -r ./requirements/requirements.docs.txt\n\n# Generate diagrams:\n./scripts/diagrams.sh\n</code></pre>","tags":["dev","development"]},{"location":"dev/docs/#mkdocs-material","title":"MkDocs Material","text":"","tags":["dev","development"]},{"location":"dev/docs/#installation","title":"Installation","text":"<pre><code># Install mkdocs-material and mkdocstrings:\npip install -U mkdocs-material mkdocstrings[python]\n</code></pre>","tags":["dev","development"]},{"location":"dev/docs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>","tags":["dev","development"]},{"location":"dev/docs/#docs-layout","title":"Docs layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>","tags":["dev","development"]},{"location":"dev/docs/#references","title":"References","text":"<ul> <li>MkDocs Documentation</li> <li>MkDocs Material Documentation</li> <li>mkdocstrings Documentation</li> </ul>","tags":["dev","development"]},{"location":"dev/file-structure/","title":"\ud83d\udcc2 File Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 .github/                # GitHub specific files\n|   \u251c\u2500\u2500 workflows/              # GitHub actions as workflows\n|   \u2514\u2500\u2500 release.yml             # Categories and labels for release notes\n\u251c\u2500\u2500 .vscode/                # VSCode specific files\n|   \u251c\u2500\u2500 extensions.json         # Recommended extensions for the workspace\n|   \u2514\u2500\u2500 settings.json           # Common VSCode settings for the workspace (e.g. formatting, linting, etc...)\n\u251c\u2500\u2500 build/                  # Build files and directories (SHOULD NOT BE COMMITTED TO REPOSITORY)\n\u251c\u2500\u2500 dist/                   # Built distributions of this project (SHOULD NOT BE COMMITTED TO REPOSITORY)\n\u251c\u2500\u2500 docs/                   # Documentation of this project\n|   \u251c\u2500\u2500 assets/                 # Any assets (images, audios, videos, js, css, html, etc...) used for the documentation\n|   \u251c\u2500\u2500 diagrams/               # Diagrams related to this project\n|   \u251c\u2500\u2500 blog/                   # Blog posts related to this project\n|   \u2514\u2500\u2500 .../                    # MkDocs pages - markdown files\n\u251c\u2500\u2500 examples/               # Example source codes of this project\n\u251c\u2500\u2500 requirements/           # Python dependency requirements for different environments\n\u251c\u2500\u2500 scripts/                # Helpful scripts to automate tasks or assist in the development process\n\u251c\u2500\u2500 site/                   # Built static site of the documentation (SHOULD NOT BE COMMITTED TO REPOSITORY)\n\u251c\u2500\u2500 src/                    # Source codes of this project\n|   \u251c\u2500\u2500 modules/                # External modules for this project\n|   |   \u251c\u2500\u2500 module_1/\n|   |   \u251c\u2500\u2500 module_2/\n|   |   \u2514\u2500\u2500 .../\n|   \u2514\u2500\u2500 infer-ci/            # Main CODEBASE of this project as a python module\n|       \u251c\u2500\u2500 __init__.py             # Initialize the module to be used as a package\n|       \u251c\u2500\u2500 __version__.py          # Version of the module (should be updated and used with each release)\n|       \u2514\u2500\u2500 ...                     # Other main python files of this module\n\u251c\u2500\u2500 templates/              # Template files (if any, e.g. config files, etc...) used in this project\n\u251c\u2500\u2500 tests/                  # Tests for this project\n|   \u251c\u2500\u2500 __init__.py             # Initialize the test module\n|   \u251c\u2500\u2500 conftest.py             # Presets for pytest (e.g. fixtures, plugins, pre/post test hooks, etc...)\n|   \u251c\u2500\u2500 test_1.py               # Test case files\n|   \u251c\u2500\u2500 test_2.py\n|   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 __init__.py             # Initialize the whole project as a python module to import from other modules\n\u251c\u2500\u2500 .editorconfig           # Editor configuration for consistent coding styles for different editors\n\u251c\u2500\u2500 .env                    # Environment variables file (SHOULD NOT BE COMMITTED TO REPOSITORY)\n\u251c\u2500\u2500 .env.example            # Example environment variables file\n\u251c\u2500\u2500 .gitignore              # Files and directories to be ignored by git (e.g. data, models, results, etc...)\n\u251c\u2500\u2500 .markdownlint.json      # Markdown linting rules\n\u251c\u2500\u2500 .pre-commit-config.yaml # Pre-commit configuration file\n\u251c\u2500\u2500 CHANGELOG.md            # List of changes for each version of the project\n\u251c\u2500\u2500 environment.yml         # Conda environment file\n\u251c\u2500\u2500 LICENSE.txt             # License file for this project\n\u251c\u2500\u2500 Makefile                # Makefile for common commands and automation\n\u251c\u2500\u2500 MANIFEST.in             # Manifest file for setuptools (to include/exclude files in the source distribution)\n\u251c\u2500\u2500 mkdocs.yml              # MkDocs configuration file\n\u251c\u2500\u2500 pyproject.toml          # PEP 518 configuration file for python packaging\n\u251c\u2500\u2500 pytest.ini              # Pytest configuration file\n\u251c\u2500\u2500 README.md               # Main README file for this project\n\u251c\u2500\u2500 requirements.txt        # Main python dependency requirements for this project\n\u251c\u2500\u2500 setup.cfg               # Configuration for setuptools\n\u2514\u2500\u2500 setup.py                # Setup script for setuptools (for backward compatibility)\n</code></pre>","tags":["dev","development"]},{"location":"dev/related-projects/","title":"\ud83d\uddc2 Related Projects","text":"<p>This section lists related projects or sub-module projects that are part of this project.</p>","tags":["dev","development"]},{"location":"dev/roadmap/","title":"\ud83d\udee4 Roadmap","text":"<p>This project is still in its early stages of development.</p> <p>The following is a list of features that are planned for future releases:</p>","tags":["dev","development"]},{"location":"dev/sitemap/","title":"\ud83d\uddfa\ufe0f Sitemap","text":"<ul> <li>Home</li> <li>Getting Started<ul> <li>Prerequisites</li> <li>Installation</li> <li>Configuration</li> <li>Examples</li> <li>Error Codes</li> </ul> </li> <li>API Documentation<ul> <li>MyClass</li> </ul> </li> <li>Development<ul> <li>Test</li> <li>Build</li> <li>Docs</li> <li>Scripts<ul> <li>clean.sh</li> <li>get-version.sh</li> <li>test.sh</li> <li>bump-version.sh</li> <li>build.sh</li> <li>release.sh</li> <li>changelog.sh</li> <li>diagrams.sh</li> <li>docs.sh</li> </ul> </li> <li>CI/CD<ul> <li>1.bump-version.yml</li> <li>2.build-publish.yml</li> <li>3.update-changelog.yml</li> <li>publish-docs.yml</li> </ul> </li> <li>Diagrams</li> <li>File Structure</li> <li>Sitemap</li> <li>Related Projects</li> <li>Contributing</li> <li>Roadmap</li> </ul> </li> <li>Research<ul> <li>Reports</li> <li>Benchmarks</li> <li>References</li> </ul> </li> <li>Release Notes</li> <li>About<ul> <li>FAQ</li> <li>Authors</li> <li>Contact</li> <li>License</li> </ul> </li> </ul>","tags":["dev","development"]},{"location":"dev/test/","title":"\ud83e\uddea Test","text":"<p>To run tests, run the following command:</p> <pre><code># Install python test dependencies:\npip install .[test]\n\n# Run tests:\npython -m pytest -sv -o log_cli=true\n# Or use the test script:\n./scripts/test.sh -l -v -c\n</code></pre>","tags":["dev","development"]},{"location":"dev/test/#pytest","title":"Pytest","text":"<pre><code># Install pytest:\npip install -U pytest pytest-cov pytest-xdist pytest-benchmark\n\n# Run tests:\npython -m pytest\n\n# Pytest help:\npython -m pytest --help\n</code></pre>","tags":["dev","development"]},{"location":"dev/test/#references","title":"References","text":"<ul> <li>Pytest Documentation</li> <li>Pytest Getting Started</li> <li>Pytest Fixtures</li> <li>Blogs:<ul> <li>https://docs.pytest.org/en/latest/goodpractices.html</li> <li>https://emimartin.me/pytest_best_practices</li> <li>https://esaezgil.com/post/unittesting_pitfalls</li> <li>https://pytest-with-eric.com/mocking/pytest-common-mocking-problems</li> </ul> </li> </ul>","tags":["dev","development"]},{"location":"dev/cicd/","title":"\ud83d\udc77 CI/CD","text":"<p>This section provides information on how to setup and configure CI/CD pipelines for this project.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/#github-actions","title":"GitHub Actions","text":"<ul> <li><code>1.bump-version.yml</code>: Bumps the project version.</li> <li><code>2.build-publish.yml</code>: Builds, publishes and creates release of python package.</li> <li><code>3.update-changelog.yml</code>: Updates the changelog.</li> <li><code>publish-docs.yml</code>: Publishes the documentation.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/#references","title":"References","text":"<ul> <li>GitHub Actions Documentation</li> <li>GitHub Actions Marketplace</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/","title":"\u2b06\ufe0f Bump Version","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#overview","title":"Overview","text":"<p>This GitHub Action automates the process of bumping the project version. It allows users to choose whether to increment the version as a patch, minor, or major release. This workflow consists of two main jobs:</p> <ol> <li>Test: Runs tests before bumping the version.</li> <li>Bump Version: Increases the project version and commits the changes.</li> </ol>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#how-it-works","title":"How It Works","text":"<p>The workflow is triggered manually via GitHub's <code>workflow_dispatch</code> event. Users must select the type of version bump before execution.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#workflow-configuration","title":"Workflow Configuration","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#trigger","title":"Trigger","text":"<p>The action is triggered manually by dispatching a workflow with an input parameter <code>bump_type</code>, which can have one of the following values:</p> <ul> <li><code>patch</code></li> <li><code>minor</code></li> <li><code>major</code></li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#jobs","title":"Jobs","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#1-test-11-test","title":"1. Test (1.1. Test)","text":"<p>This job ensures the code is tested before making version changes.</p> <ul> <li>Runs on: <code>ubuntu-24.04</code></li> <li>Permissions: <code>contents: read</code></li> <li>Steps:<ol> <li>Checkout the repository</li> <li>Install dependencies (from <code>requirements/requirements.test.txt</code>)</li> <li>Run tests using <code>pytest</code> via <code>./scripts/test.sh -l</code></li> </ol> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#2-bump-version-12-bump-version","title":"2. Bump Version (1.2. Bump Version)","text":"<p>This job updates the project version after tests pass.</p> <ul> <li>Runs on: <code>ubuntu-24.04</code></li> <li>Permissions: <code>contents: write</code></li> <li>Steps:<ol> <li>Checkout the repository (with full history)</li> <li>Bump the version using <code>./scripts/bump-version.sh</code></li> <li>Commits and pushes changes using GitHub Actions bot</li> </ol> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#usage","title":"Usage","text":"<p>To manually trigger this workflow:</p> <ol> <li>Navigate to the repository on GitHub.</li> <li>Go to the Actions tab.</li> <li>Select \"1. Bump Version\" from the list.</li> <li>Click \"Run workflow\".</li> <li>Choose a version bump type (<code>patch</code>, <code>minor</code>, or <code>major</code>).</li> <li>Click \"Run workflow\" to start the process.</li> </ol>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#environment-variables","title":"Environment Variables","text":"<p>The workflow uses the following environment variables:</p> <ul> <li><code>GITHUB_TOKEN</code>: GitHub-provided authentication token for making commits.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#scripts-used","title":"Scripts Used","text":"<ul> <li><code>test.sh</code>: Runs the test suite.</li> <li><code>bump-version.sh</code>: Handles version incrementing.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>bump-version.sh</code> supports <code>-b</code>, <code>-c</code>, and <code>-p</code> options.</li> <li>The workflow ensures that version bumping occurs only if tests pass.</li> <li>The changes are committed and pushed automatically to the repository.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/1.bump-version/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the workflow fails in the <code>test</code> step, check test logs for errors.</li> <li>If version bumping fails, ensure <code>bump-version.sh</code> is executable and correctly configured.</li> <li>If permissions errors occur, verify that GitHub Actions has the required <code>contents: write</code> permission.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/","title":"\ud83d\ude80 Build and Publish","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#overview","title":"Overview","text":"<p>This GitHub Action automates the process of building and publishing the project. It triggers automatically after a version bump or when a tag (<code>v*.*.*</code>) is pushed. The workflow builds the package and optionally publishes it to a package registry.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#how-it-works","title":"How It Works","text":"<p>The workflow runs in the following scenarios:</p> <ul> <li>After Bump Version workflow completes.</li> <li>When a new tag (<code>v*.*.*</code>) is pushed to the repository.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#workflow-configuration","title":"Workflow Configuration","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#trigger","title":"Trigger","text":"<ul> <li>Triggered by:<ul> <li>Completion of <code>1. Bump Version</code> workflow.</li> <li>Push event on tags matching <code>v*.*.*</code>.</li> </ul> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#jobs","title":"Jobs","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#1-build-and-publish","title":"1. Build and Publish","text":"<p>This job builds the package and creates a release.</p> <ul> <li>Runs on: <code>ubuntu-24.04</code></li> <li>Permissions: <code>contents: write</code></li> <li>Steps:<ol> <li>Checkout the repository</li> <li>Install dependencies (from <code>requirements/requirements.build.txt</code>)</li> <li>Build the package using <code>./scripts/build.sh -c</code></li> <li>Create a release using GitHub CLI (<code>gh release create</code>)</li> </ol> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#usage","title":"Usage","text":"<p>This workflow runs automatically when a new version is tagged. However, you can manually trigger a tag and push it:</p> <ol> <li>Bump the version using the <code>1. Bump Version</code> workflow.</li> <li> <p>Create a tag manually and push it:</p> <pre><code>git tag v1.2.3\ngit push origin v1.2.3\n</code></pre> </li> <li> <p>The workflow will build and publish the package.</p> </li> </ol>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>GITHUB_TOKEN</code>: Used for creating GitHub releases.</li> <li><code>PYPI_API_TOKEN</code> (if enabled): Used for publishing packages to PyPI.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#scripts-used","title":"Scripts Used","text":"<ul> <li><code>build.sh</code>: Builds the package.</li> <li><code>get-version.sh</code>: Retrieves the current version.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#notes","title":"Notes","text":"<ul> <li>This workflow supports both GitHub Releases and optional package publishing.</li> <li>Ensure <code>build.sh</code> is executable and correctly configured.</li> <li>If the workflow fails, check logs for errors related to dependencies or authentication.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/2.build-publish/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the build step fails, ensure dependencies are correctly installed.</li> <li>If release creation fails, verify that <code>GITHUB_TOKEN</code> has the necessary permissions.</li> <li>If publishing to PyPI fails, check that the API token is correctly set up in repository secrets.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/","title":"\ud83d\udca5 Update Changelog","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#overview","title":"Overview","text":"<p>This GitHub Action automates the process of updating the changelog after a new release is built and published. It ensures that the changelog remains up to date with the latest changes in the repository.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#how-it-works","title":"How It Works","text":"<p>The workflow is triggered automatically after the Build and Publish workflow is successfully completed.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#workflow-configuration","title":"Workflow Configuration","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#trigger","title":"Trigger","text":"<ul> <li>Triggered by:<ul> <li>Completion of <code>2. Build and Publish</code> workflow.</li> </ul> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#jobs","title":"Jobs","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#1-update-changelog","title":"1. Update Changelog","text":"<p>This job updates the changelog after a new release.</p> <ul> <li>Runs on: <code>ubuntu-24.04</code></li> <li>Permissions: <code>contents: write</code></li> <li>Steps:<ol> <li>Checkout the repository (with full history for changelog updates)</li> <li>Update the changelog using <code>./scripts/changelog.sh -c -p</code></li> <li>Commits and pushes changes using GitHub Actions bot</li> </ol> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#usage","title":"Usage","text":"<p>This workflow runs automatically when a new release is created. However, you can manually update the changelog by running the script locally:</p> <pre><code>./scripts/changelog.sh -c -p\n</code></pre>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>GITHUB_TOKEN</code>: Used for authentication and committing changes.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#scripts-used","title":"Scripts Used","text":"<ul> <li><code>changelog.sh</code>: Updates the changelog file.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>changelog.sh</code> is executable and correctly configured.</li> <li>The workflow ensures that changelog updates occur only after a successful release build.</li> <li>The changes are committed and pushed automatically to the repository.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/3.update-changelog/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the workflow fails, ensure that <code>changelog.sh</code> is present and executable.</li> <li>If there are permissions errors, verify that GitHub Actions has the required <code>contents: write</code> permission.</li> <li>Check logs for any issues related to Git authentication or script execution.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/","title":"\ud83d\udcdd Publish Docs","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#overview","title":"Overview","text":"<p>This GitHub Action automates the process of publishing documentation using MkDocs whenever changes are pushed to the <code>main</code> branch. It ensures that the latest documentation is deployed automatically.</p>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#how-it-works","title":"How It Works","text":"<p>The workflow is triggered when changes are made to:</p> <ul> <li>The <code>docs/</code> directory.</li> <li>The <code>mkdocs.yml</code> configuration file.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#workflow-configuration","title":"Workflow Configuration","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#trigger","title":"Trigger","text":"<ul> <li>Triggered by:<ul> <li>A push event to the <code>main</code> branch that modifies documentation files.</li> </ul> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#jobs","title":"Jobs","text":"","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#1-publish-docs","title":"1. Publish Docs","text":"<p>This job builds and deploys the documentation using MkDocs.</p> <ul> <li>Runs on: <code>ubuntu-24.04</code></li> <li>Permissions: <code>contents: write</code></li> <li>Steps:<ol> <li>Checkout the repository (with full history for proper deployment tracking).</li> <li>Install dependencies from <code>requirements/requirements.docs.txt</code></li> <li>Publish the documentation using <code>mkdocs gh-deploy --force</code></li> </ol> </li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#usage","title":"Usage","text":"<p>This workflow runs automatically when changes are pushed to the <code>main</code> branch. However, you can manually deploy the documentation by running the following command locally:</p> <pre><code>mkdocs gh-deploy --force\n</code></pre>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>GITHUB_TOKEN</code>: Used for authentication and deploying the documentation.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#scripts-used","title":"Scripts Used","text":"<ul> <li><code>mkdocs.yml</code>: Configuration file for MkDocs.</li> <li>Files in <code>docs/</code>: Documentation content.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>mkdocs</code> and required dependencies are correctly installed in <code>requirements.docs.txt</code>.</li> <li>The workflow ensures that documentation updates occur only when relevant files are modified.</li> <li>The changes are committed and published automatically to GitHub Pages.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/cicd/publish-docs/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the workflow fails, ensure that <code>mkdocs</code> is properly installed and configured.</li> <li>If deployment issues occur, verify that GitHub Actions has the required <code>contents: write</code> permission.</li> <li>Check logs for any issues related to Git authentication or MkDocs execution.</li> </ul>","tags":["dev","development","cicd"]},{"location":"dev/scripts/","title":"\ud83d\udd28 Scripts","text":"<p>This document provides an overview and usage instructions for the following scripts in this project:</p> <ul> <li><code>clean.sh</code></li> <li><code>get-version.sh</code></li> <li><code>test.sh</code></li> <li><code>bump-version.sh</code></li> <li><code>build.sh</code></li> <li><code>release.sh</code></li> <li><code>changelog.sh</code></li> <li><code>diagrams.sh</code></li> <li><code>docs.sh</code></li> </ul> <p>All the scripts are located in the <code>scripts</code> directory:</p> <pre><code>scripts/\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 bump-version.sh\n\u251c\u2500\u2500 changelog.sh\n\u251c\u2500\u2500 clean.sh\n\u251c\u2500\u2500 diagrams.sh\n\u251c\u2500\u2500 docs.sh\n\u251c\u2500\u2500 get-version.sh\n\u251c\u2500\u2500 release.sh\n\u2514\u2500\u2500 test.sh\n</code></pre> <p>These scripts are designed to be used in a Linux or macOS environment. They may work in a Windows environment with the appropriate tools installed, but this is not guaranteed.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/build.sh/","title":"\ud83c\udfd7\ufe0f build.sh","text":"<p>This script is used to build a Python project and optionally run tests and publish the package. It also includes a cleaning operation to clear the build directories.</p> <p>This script has the following key features:</p> <ul> <li>Checking for required tools: It verifies if Python and the build package are installed on the system. If tests are not disabled, it also checks if pytest is installed. If uploading is enabled, it checks for the presence of Twine.</li> <li>Command-line argument parsing: It parses <code>-c</code> or <code>--disable-clean</code> to disable cleaning the build directories, <code>-t</code> or <code>--test</code> to enable running tests, <code>-u</code> or <code>--upload</code> to enable publishing the package, and <code>-p</code> or <code>--production</code> to switch the package repository from staging (default) to production.</li> <li>Clean operation: Cleans the build directories before and after building (if enabled). If <code>-c</code> or <code>--disable-clean</code> is passed, the script will not clean the build directories.</li> <li>Testing operation: Runs pytest tests if enabled by <code>-t</code> or <code>--test</code> flag.</li> <li>Build operation: Builds a Python package using the Python build package.</li> <li>Publishing operation: Publishes the built package to a PyPi repository using Twine if the <code>-u</code> or <code>--upload</code> flag is passed. Defaults to the TestPyPi (staging) repository, but can be switched to the production (PyPi) repository using the <code>-p</code> or <code>--production</code> flag.</li> </ul> <p>Usage:</p> <p>To execute the build script with the different flags, use the following commands:</p> <pre><code>./build.sh [-c|--disable-clean] [-t|--test] [-u|--upload] [-p|--production]\n</code></pre> <p>Examples:</p> <ul> <li>To build without cleaning: <code>./build.sh -c</code></li> <li>To build with running tests: <code>./build.sh -t</code></li> <li>To build and publish to the staging repository: <code>./build.sh -u</code></li> <li>To build and publish to the production repository: <code>./build.sh -u -p</code></li> </ul> <p>This script is particularly beneficial for developers, streamlining the build, testing, and publishing process. It provides a one-stop solution for all the build needs of a Python project, reducing chances of errors and ensuring consistency.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/bump-version.sh/","title":"\ud83c\udff7 bump-version.sh","text":"<p>This script is used to manage the versioning of the project. It allows you to increment the major, minor, or patch part of the version, as per Semantic Versioning rules.</p> <p>The script carries out the following operations:</p> <ul> <li>Loading environment variables: If a <code>.env</code> file is present in the root directory, the script loads the environment variables from this file.</li> <li>Sets variables: Sets the <code>VERSION_FILE_PATH</code> and other variables. The <code>VERSION_FILE_PATH</code> variable is either loaded from the environment or defaults to <code>src/infer-ci/__version__.py</code>.</li> <li>Parses input arguments: It parses the <code>-b</code> or <code>--bump-type</code> argument for the type of version bump (<code>major</code>, <code>minor</code>, or <code>patch</code>) and <code>-p</code> or <code>--push-tag</code> to decide whether to push the tag to the Git repository or not.</li> <li>Checks and increments the version: It uses <code>get-version.sh</code> to extract the current version from the file specified by <code>VERSION_FILE_PATH</code>. Based on the bump type, it increments the appropriate part of the version and writes the new version back to the file.</li> <li>Commits and tags: If the <code>-p</code> or <code>--push-tag</code> flag was provided, it adds and commits the changes, pushes the changes, creates a new tag with the new version, and pushes the tag to the Git repository. It will prevent the operation if the tag already exists.</li> </ul> <p>Usage:</p> <p>To execute the bump version script, run the following command in the terminal:</p> <pre><code>./bump-version.sh -b=&lt;bump_type&gt; -p\n</code></pre> <p>Replace <code>&lt;bump_type&gt;</code> with either <code>major</code>, <code>minor</code>, or <code>patch</code> to indicate which part of the version to increment. The <code>-p</code> or <code>--push-tag</code> flag tells the script to commit the changes and push the tag to the Git repository.</p> <p>Examples:</p> <p>To bump the <code>minor</code> version and push the new tag, run:</p> <pre><code>./bump-version.sh -b=minor -p\n</code></pre> <p>This script streamlines the versioning process, reducing the chances of errors and ensuring consistency in versioning.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/bump-version.sh/#references","title":"References","text":"<ul> <li>https://semver.org</li> </ul>","tags":["dev","development","scripts"]},{"location":"dev/scripts/changelog.sh/","title":"\ud83d\udccc changelog.sh","text":"<p>This script automates updating both a <code>CHANGELOG.md</code> file and a <code>docs/release-notes.md</code> file using release information fetched from GitHub. It ensures consistent release documentation across the project and optionally commits and pushes changes to the Git repository.</p> <p>The script performs the following operations:</p> <ul> <li>Environment setup:  <ul> <li>Runs from the project root.  </li> <li>Loads environment variables from a <code>.env</code> file if present.  </li> </ul> </li> <li>Dependency checks:  <ul> <li>Ensures the GitHub CLI (<code>gh</code>) is installed and authenticated.  </li> <li>If <code>--commit</code> is specified, verifies that <code>git</code> is available.  </li> </ul> </li> <li>Variables setup:  <ul> <li><code>CHANGELOG_FILE_PATH</code> \u2192 Path to the changelog file (default: <code>./CHANGELOG.md</code>).  </li> <li><code>RELEASE_NOTES_FILE_PATH</code> \u2192 Path to the release notes file (default: <code>./docs/release-notes.md</code>).  </li> </ul> </li> <li>Input parsing:  <ul> <li><code>-c</code> or <code>--commit</code>: Commit changelog and release notes updates.  </li> <li><code>-p</code> or <code>--push</code>: Push updates to the remote repository (requires <code>-c</code>).  </li> </ul> </li> <li>Changelog update:  <ul> <li>Fetches the latest release tag and body from GitHub (<code>gh release view</code>).  </li> <li>Updates <code>CHANGELOG.md</code> with a new section for the latest release, including date and notes.  </li> </ul> </li> <li>Release notes update:  <ul> <li>Updates <code>docs/release-notes.md</code> with a formatted entry for the latest release.  </li> <li>Adds a YAML front matter block and header if the file does not already exist.  </li> </ul> </li> <li>Commit and push (optional):  <ul> <li>If <code>-c</code> is provided, stages and commits both updated files with a commit message.  </li> <li>If <code>-p</code> is also provided, pushes the commit to the remote repository.</li> </ul> </li> </ul>","tags":["dev","development","scripts"]},{"location":"dev/scripts/changelog.sh/#usage","title":"Usage","text":"<p>To execute <code>changelog.sh</code>, run:</p> <pre><code>./changelog.sh [-c|--commit] [-p|--push]\n</code></pre>","tags":["dev","development","scripts"]},{"location":"dev/scripts/clean.sh/","title":"\ud83e\uddf9 clean.sh","text":"<p>This script is designed to clean up the build environment by removing artifacts and other temporary or unwanted files and directories.</p> <p>The script performs the following operations:</p> <ul> <li>Delete system files: Finds and deletes all <code>.DS_Store</code> and <code>.Thumbs.db</code> files in the project directory and its subdirectories.</li> <li>Delete cache directories: Finds and deletes all <code>__pycache__</code> directories in the project directory and its subdirectories.</li> <li>Delete project-related directories: Removes directories created during the test and build process or by tools used in the project, such as <code>.benchmarks</code>, <code>.pytest_cache</code>, <code>build</code>, and <code>dist</code> directories.</li> <li>Delete <code>.coverage</code> file: Removes the <code>.coverage</code> file that's created when coverage information is collected for the project.</li> </ul> <p>Usage:</p> <p>To execute the clean script, simply run the following command in the terminal:</p> <pre><code>./clean.sh [-a|--all]\n</code></pre> <p>Examples:</p> <ul> <li>To clean just non-essential files: <code>./clean.sh</code></li> <li>To clean all files: <code>./clean.sh -a</code></li> </ul> <p>This will clean up the project directory, removing any unnecessary files and directories and ensuring a clean environment for a fresh build.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/diagrams.sh/","title":"\ud83d\uddbc\ufe0f diagrams.sh","text":"<p>This script generates UML diagrams for a specified Python module using <code>pyreverse</code>. It checks for dependencies, loads environment variables, sets directories, and processes command-line arguments for customization of module names, directories, and output locations.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/diagrams.sh/#overview","title":"Overview","text":"<p>The script performs the following operations:</p> <ul> <li> <p>Dependency Checks: Verifies the presence of required tools:</p> <ul> <li><code>graphviz</code> for <code>.dot</code> file handling.</li> <li><code>python</code> for running Python-based commands.</li> <li><code>pylint</code> (with <code>pyreverse</code>) for generating UML diagrams.</li> </ul> </li> <li> <p>Environment Variable Setup: Sets default values for module name, module directory, and output directory (<code>MODULE_NAME</code>, <code>MODULE_DIR</code>, <code>OUTPUT_DIR</code>). These can be customized via environment variables or command-line arguments.</p> </li> <li> <p>Argument Parsing: Parses optional arguments to allow customization:</p> <ul> <li><code>-m</code> or <code>--module-name</code> to specify the module name.</li> <li><code>-d</code> or <code>--module-dir</code> to specify the module directory.</li> <li><code>-o</code> or <code>--output-dir</code> to specify the output directory.</li> </ul> </li> <li> <p>Directory Creation: Creates subdirectories within the output directory for organizing different types of UML and flowchart outputs:</p> <ul> <li><code>classes</code> for class diagrams.</li> <li><code>packages</code> for package diagrams.</li> </ul> </li> <li> <p>Diagram Generation: Runs <code>pyreverse</code> to create UML diagrams in multiple formats (<code>html</code>, <code>pdf</code>, <code>png</code>, and <code>svg</code>) and organizes them into respective directories.</p> </li> <li> <p>Completion Message: Displays a message confirming successful generation of diagrams.</p> </li> </ul>","tags":["dev","development","scripts"]},{"location":"dev/scripts/diagrams.sh/#usage","title":"Usage","text":"<p>To run the script:</p> <pre><code>./diagrams.sh -m=&lt;module_name&gt; -d=&lt;module_dir&gt; -o=&lt;output_dir&gt;\n</code></pre>","tags":["dev","development","scripts"]},{"location":"dev/scripts/docs.sh/","title":"\ud83d\udcdd docs.sh","text":"<p>This script is used to manage the documentation for the project, providing options to either serve a local documentation server or build the documentation as static HTML files.</p> <p>The script performs the following operations:</p> <ul> <li>Serving documentation: If no flags are set, runs <code>mkdocs serve</code> to start a local documentation server for live preview.</li> <li>Building documentation: If the <code>-b</code> or <code>--build</code> flag is set, the script builds the documentation as static HTML files using <code>mkdocs build</code>, placing the output in the <code>site</code> directory.</li> <li>Publishing documentation: If the <code>-p</code> or <code>--publish</code> flag is set, the script can be extended to publish the documentation to GitHub Pages.</li> </ul> <p>Usage:</p> <p>To execute the documentation script, use the following command in the terminal:</p> <pre><code>./docs.sh [-b|--build] [-p|--publish]\n</code></pre> <p>Examples:</p> <ul> <li>To serve the documentation: <code>./docs.sh</code></li> <li>To build the documentation: <code>./docs.sh -b</code></li> <li>To publish the documentation: <code>./docs.sh -p</code></li> </ul>","tags":["dev","development","scripts"]},{"location":"dev/scripts/get-version.sh/","title":"\ud83d\udd0d get-version.sh","text":"<p>This script is used to retrieve the current version of the application from a specified version file.</p> <p>The script performs the following operations:</p> <ul> <li><code>VERSION_FILE_PATH</code> is either loaded from the environment or, if it's not present in the environment, it defaults to <code>src/infer-ci/__version__.py</code>.</li> <li>It first checks if the <code>VERSION_FILE_PATH</code> variable is not empty and if the file exists. If these conditions are met, it retrieves the value of <code>__version__</code> from the file by using <code>grep</code>, <code>awk</code>, and <code>tr</code> commands. The <code>grep</code> command filters the line containing <code>__version__ =</code> , the <code>awk</code> command splits the line into two parts at <code>=</code>, and the <code>tr</code> command removes the quotes around the version. If these operations fail, it exits the script with status code <code>2</code>.</li> <li>If the <code>VERSION_FILE_PATH</code> variable is empty or the file does not exist, it sets the current version to <code>0.0.0</code>.</li> <li>Finally, it echoes the current version to the console.</li> </ul> <p>Usage:</p> <p>To execute the get version script, simply run the following command in the terminal:</p> <pre><code>./get-version.sh\n</code></pre> <p>This script can be used to conveniently fetch the version. It is used by the <code>bump-version.sh</code> script to retrieve the current version before incrementing it.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/release.sh/","title":"\ud83d\ude80 release.sh","text":"<p>This script automates the creation of GitHub Releases for the project. It optionally performs a build, retrieves the current version, and uploads artifacts from the <code>dist</code> directory to a new GitHub Release with autogenerated notes.</p> <p>The script performs the following operations:</p> <ul> <li>Environment setup:   Ensures it runs from the project root and sources environment variables from <code>.env</code> if available.</li> <li>Dependency checks:   Verifies that <code>git</code> and <code>gh</code> (GitHub CLI) are installed, and that the user is authenticated with <code>gh auth login</code>.</li> <li>Optional build:   If the <code>-b</code> or <code>--build</code> flag is set, runs <code>./scripts/build.sh -c</code> before release.</li> <li>Versioning:   Uses <code>./scripts/get-version.sh</code> to determine the release version.</li> <li>Release creation:   Runs <code>gh release create v&lt;version&gt; ./dist/* --generate-notes</code> to publish a new GitHub Release with attached artifacts.</li> </ul> <p>Usage:</p> <p>To execute the release script, use the following command in the terminal:</p> <pre><code>./release.sh [-b|--build]\n</code></pre> <p>Examples:</p> <ul> <li>To create a release using existing build artifacts: <code>./release.sh</code></li> <li>To build the project first, then create the release: <code>./release.sh -b</code></li> </ul> <p>Notes:</p> <ul> <li>A .env file is optional but will be loaded if present.</li> <li>The dist/ directory must contain the build artifacts before release.</li> <li>The release tag will be prefixed with v (e.g., v1.2.3).</li> </ul>","tags":["dev","development","scripts"]},{"location":"dev/scripts/test.sh/","title":"\ud83e\uddea test.sh","text":"<p>This script is used to run the pytest tests for the project.</p> <p>The script performs the following operations:</p> <ul> <li>Running pytest: Runs the pytest tests for the project.</li> <li>Logging: If the <code>-l</code> or <code>--log</code> option is provided, the script will log the output of the pytest tests to console.</li> <li>Coverage: If the <code>-c</code> or <code>--cov</code> option is provided, the script will run the pytest tests with coverage.</li> <li>Verbose: If the <code>-v</code> or <code>--verbose</code> option is provided, the script will run the pytest tests with verbose error outputs.</li> </ul> <p>Usage:</p> <p>To execute the test script, simply run the following command in the terminal:</p> <pre><code>./test.sh [-l|--log] [-c|--cov] [-v|--verbose]\n</code></pre> <p>Examples:</p> <ul> <li>To test: <code>./test.sh</code></li> <li>To test with logging: <code>./test.sh -l</code></li> <li>To test with coverage: <code>./test.sh -c</code></li> <li>To test with verbose: <code>./test.sh -v</code></li> <li>To test with logging, coverage and verbose: <code>./test.sh -l -c -v</code></li> </ul> <p>This script will run the pytest tests for the project. It can also be used to run the tests with logging, coverage, and verbose options.</p>","tags":["dev","development","scripts"]},{"location":"dev/scripts/test.sh/#references","title":"References","text":"<ul> <li>https://docs.pytest.org</li> </ul>","tags":["dev","development","scripts"]},{"location":"getting-started/configuration/","title":"\u2699\ufe0f Configuration","text":"<p><code>templates/configs/config.yml</code>:</p> <pre><code>infer-ci:\n  min_length: 2\n  max_length: 100\n  min_value: 0.0\n  max_value: 1.0\n  threshold: 0.5\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/configuration/#environment-variables","title":"\ud83c\udf0e Environment Variables","text":"<p><code>.env.example</code>:</p> <pre><code># ENV=LOCAL\n# DEBUG=false\n# TZ=UTC\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/error-codes/","title":"\ud83d\udea8 Error Codes","text":"","tags":["getting-started"]},{"location":"getting-started/error-codes/#error-handling","title":"Error Handling","text":"<p>Pydantic will raise a <code>ValidationError</code> whenever it finds an error in the data it's validating.</p> <pre><code>from pydantic import ValidationError\n\nfrom infer-ci import MyClass\n\n\ntry:\n    _my_object = MyClass(\n        config={\n            \"min_length\": 0,\n            \"max_length\": \"three\",\n            \"min_value\": True,\n            \"max_value\": [1, 2, 3],\n            \"threshold\": 2.0,\n        }\n    )\nexcept ValidationError as err:\n    print(err)\n</code></pre> <p>The error message will look like this:</p> <pre><code>4 validation errors for MyClassConfigPM\nmin_length\n  Input should be greater than or equal to 1 [type=greater_than_equal, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/greater_than_equal\nmax_length\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='three', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/int_parsing\nmax_value\n  Input should be a valid number [type=float_type, input_value=[1, 2, 3], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_type\nthreshold\n  Input should be less than or equal to 1 [type=less_than_equal, input_value=2.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.10/v/less_than_equal\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/examples/","title":"\ud83d\udeb8 Examples","text":"","tags":["getting-started"]},{"location":"getting-started/examples/#simple","title":"Simple","text":"<p><code>examples/simple/main.py</code>:</p> <pre><code>#!/usr/bin/env python\n\"\"\"\nSimple Binary Classification Example with Confidence Intervals\n\nThis example demonstrates how to compute classification metrics with\nconfidence intervals using the infer-ci package.\n\"\"\"\n\nimport numpy as np\nfrom infer_ci import MetricEvaluator, accuracy_score\n\ndef main():\n    print(\"Binary Classification with Confidence Intervals\")\n    print(\"=\" * 50)\n\n    # Sample binary classification data\n    y_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n    y_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n\n    print(f\"\\nDataset size: {len(y_true)} samples\")\n    print(f\"True labels:      {y_true}\")\n    print(f\"Predicted labels: {y_pred}\")\n\n    # Method 1: Using MetricEvaluator (Recommended)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Method 1: Using MetricEvaluator\")\n    print(\"=\" * 50)\n\n    evaluator = MetricEvaluator()\n\n    # Compute accuracy with Wilson score interval\n    acc, ci = evaluator.evaluate(\n        y_true=y_true,\n        y_pred=y_pred,\n        task='classification',\n        metric='accuracy',\n        method='wilson'\n    )\n    print(f\"\\nAccuracy: {acc:.3f}\")\n    print(f\"95% CI (Wilson): [{ci[0]:.3f}, {ci[1]:.3f}]\")\n\n    # Compute F1 score with bootstrap\n    f1, ci_f1 = evaluator.evaluate(\n        y_true=y_true,\n        y_pred=y_pred,\n        task='classification',\n        metric='f1',\n        method='bootstrap_bca',\n        n_resamples=1000\n    )\n    print(f\"\\nF1 Score: {f1:.3f}\")\n    print(f\"95% CI (Bootstrap BCA): [{ci_f1[0]:.3f}, {ci_f1[1]:.3f}]\")\n\n    # Method 2: Direct metric function call\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Method 2: Direct Function Call\")\n    print(\"=\" * 50)\n\n    acc2, ci2 = accuracy_score(y_true, y_pred, method='wilson')\n    print(f\"\\nAccuracy: {acc2:.3f}\")\n    print(f\"95% CI: [{ci2[0]:.3f}, {ci2[1]:.3f}]\")\n\n    # Compute multiple metrics at once\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Computing Multiple Metrics\")\n    print(\"=\" * 50)\n\n    metrics = ['accuracy', 'precision', 'recall', 'f1']\n    for metric in metrics:\n        value, ci = evaluator.evaluate(\n            y_true=y_true,\n            y_pred=y_pred,\n            task='classification',\n            metric=metric,\n            method='wilson'\n        )\n        print(f\"{metric.capitalize():12s}: {value:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Done!\")\n    print(\"=\" * 50)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/installation/","title":"\ud83d\udee0 Installation","text":"","tags":["getting-started"]},{"location":"getting-started/installation/#1-download-or-clone-the-repository","title":"1. \ud83d\udce5 Download or clone the repository","text":"<p>[TIP] Skip this step, if you're going to install the package directly from PyPi or GitHub repository.</p> <p>1.1. Prepare projects directory (if not exists):</p> <pre><code># Create projects directory:\nmkdir -pv ~/workspaces/projects\n\n# Enter into projects directory:\ncd ~/workspaces/projects\n</code></pre> <p>1.2. Follow one of the below options [A], [B] or [C]:</p> <p>OPTION A. Clone the repository:</p> <pre><code>git clone https://github.com/humblebeeai/infer-ci.git &amp;&amp; \\\n    cd infer-ci\n</code></pre> <p>OPTION B. Clone the repository (for DEVELOPMENT: git + ssh key):</p> <pre><code>git clone git@github.com:humblebeeai/infer-ci.git &amp;&amp; \\\n    cd infer-ci\n</code></pre> <p>OPTION C. Download source code:</p> <ol> <li>Download archived zip file from releases.</li> <li>Extract it into the projects directory.</li> </ol>","tags":["getting-started"]},{"location":"getting-started/installation/#2-install-the-package","title":"2. \ud83d\udce6 Install the package","text":"<p>[NOTE] Choose one of the following methods to install the package [A ~ F]:</p> <p>OPTION A. [RECOMMENDED] Install from PyPi:</p> <pre><code># Install from staging TestPyPi:\npip install -i https://test.pypi.org/simple -U infer-ci\n\n# Or install from production PyPi:\n# pip install -U infer-ci\n</code></pre> <p>OPTION B. Install latest version directly from GitHub repository:</p> <pre><code>pip install git+https://github.com/humblebeeai/infer-ci.git\n</code></pre> <p>OPTION C. Install from the downloaded source code:</p> <pre><code># Install directly from the source code:\npip install .\n\n# Or install with editable mode:\npip install -e .\n</code></pre> <p>OPTION D. Install for DEVELOPMENT environment:</p> <pre><code>pip install -e .[dev]\n\n# Install pre-commit hooks:\npre-commit install\n</code></pre> <p>OPTION E. Install from pre-built release files:</p> <ol> <li>Download <code>.whl</code> or <code>.tar.gz</code> file from releases</li> <li>Install with pip:</li> </ol> <pre><code># Install from .whl file:\npip install ./infer-ci-[VERSION]-py3-none-any.whl\n\n# Or install from .tar.gz file:\npip install ./infer-ci-[VERSION].tar.gz\n</code></pre> <p>OPTION F. Copy the module into the project directory (for testing):</p> <pre><code># Install python dependencies:\npip install -r ./requirements.txt\n\n# Copy the module source code into the project:\ncp -r ./src/infer-ci [PROJECT_DIR]\n# For example:\ncp -r ./src/infer-ci /some/path/project/\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/prerequisites/","title":"\ud83d\udea7 Prerequisites","text":"<ul> <li>Install Python (&gt;= v3.10) and pip (&gt;= 23):<ul> <li>[RECOMMENDED]  Miniconda (v3)</li> <li>[arm64/aarch64]  Miniforge (v3)</li> <li>[Python virutal environment]  venv</li> </ul> </li> </ul> <p>[OPTIONAL] For DEVELOPMENT environment:</p> <ul> <li>Install git</li> <li>Setup an SSH key</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with infer-ci in 5 minutes.</p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install infer-ci\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/quickstart/#your-first-evaluation","title":"Your First Evaluation","text":"","tags":["getting-started"]},{"location":"getting-started/quickstart/#classification-example","title":"Classification Example","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\n# Sample binary classification data\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1])\n\n# Initialize evaluator\nevaluator = MetricEvaluator()\n\n# Compute accuracy with Wilson interval\naccuracy, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='classification',\n    metric='accuracy',\n    method='wilson'\n)\n\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>Accuracy: 0.900\n95% CI: [0.597, 0.983]\n</code></pre></p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#regression-example","title":"Regression Example","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\n# Sample regression data\ny_true = np.array([3.0, -0.5, 2.0, 7.0, 5.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0, 4.5])\n\n# Initialize evaluator\nevaluator = MetricEvaluator()\n\n# Compute MAE with bootstrap\nmae, ci = evaluator.evaluate(\n    y_true=y_true,\n    y_pred=y_pred,\n    task='regression',\n    metric='mae',\n    method='bootstrap_bca',\n    n_resamples=2000\n)\n\nprint(f\"MAE: {mae:.3f}\")\nprint(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>MAE: 0.600\n95% CI: [0.200, 1.100]\n</code></pre></p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#alternative-direct-function-calls","title":"Alternative: Direct Function Calls","text":"<p>You can also use metric functions directly:</p> <pre><code>from infer_ci import accuracy_score, mae\nimport numpy as np\n\n# Classification\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 1, 0, 0])\n\nacc, ci = accuracy_score(y_true, y_pred, method='wilson')\nprint(f\"Accuracy: {acc:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n\n# Regression\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])\n\nmae_val, ci = mae(y_true, y_pred)\nprint(f\"MAE: {mae_val:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":"","tags":["getting-started"]},{"location":"getting-started/quickstart/#1-evaluating-multiple-metrics","title":"1. Evaluating Multiple Metrics","text":"<pre><code>from infer_ci import MetricEvaluator\nimport numpy as np\n\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1])\n\nevaluator = MetricEvaluator()\n\nmetrics = ['accuracy', 'precision', 'recall', 'f1']\n\nfor metric in metrics:\n    value, ci = evaluator.evaluate(\n        y_true=y_true,\n        y_pred=y_pred,\n        task='classification',\n        metric=metric,\n        method='wilson'\n    )\n    print(f\"{metric.capitalize():12s}: {value:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre> <p>Output: <pre><code>Accuracy    : 0.900 [0.597, 0.983]\nPrecision   : 0.857 [0.485, 0.981]\nRecall      : 1.000 [0.661, 1.000]\nF1          : 0.923 [0.640, 0.989]\n</code></pre></p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#2-comparing-ci-methods","title":"2. Comparing CI Methods","text":"<pre><code>from infer_ci import accuracy_score\nimport numpy as np\n\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n\nmethods = ['wilson', 'normal', 'bootstrap_bca']\n\nfor method in methods:\n    acc, ci = accuracy_score(y_true, y_pred, method=method, n_resamples=2000)\n    width = ci[1] - ci[0]\n    print(f\"{method:15s}: [{ci[0]:.3f}, {ci[1]:.3f}] (width: {width:.3f})\")\n</code></pre> <p>Output: <pre><code>wilson         : [0.519, 0.938] (width: 0.419)\nnormal         : [0.596, 1.004] (width: 0.408)\nbootstrap_bca  : [0.533, 0.933] (width: 0.400)\n</code></pre></p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#3-multi-class-classification","title":"3. Multi-Class Classification","text":"<pre><code>from infer_ci import classification_report_with_ci\nimport numpy as np\n\ny_true = [0, 1, 2, 2, 2, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1, 1]\ny_pred = [0, 1, 0, 0, 2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 1, 2, 2, 1, 1]\n\nclassification_report_with_ci(y_true, y_pred)\n</code></pre> <p>Output: <pre><code>     Class  Precision  Recall  F1-Score    Precision CI       Recall CI     F1-Score CI  Support\n0  Class 0      0.600   1.000     0.750  (0.231, 0.882)    (0.439, 1.0)  (0.408, 1.092)        3\n1  Class 1      0.889   1.000     0.941   (0.565, 0.98)    (0.676, 1.0)  (0.796, 1.086)        8\n2  Class 2      1.000   0.667     0.800     (0.61, 1.0)  (0.354, 0.879)  (0.562, 1.038)        9\n3    micro      0.850   0.850     0.850  (0.694, 1.006)  (0.694, 1.006)  (0.694, 1.006)       20\n4    macro      0.830   0.889     0.830  (0.702, 0.958)  (0.775, 1.002)  (0.548, 1.113)       20\n</code></pre></p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#available-metrics","title":"Available Metrics","text":"","tags":["getting-started"]},{"location":"getting-started/quickstart/#classification-metrics","title":"Classification Metrics","text":"<pre><code>from infer_ci import MetricEvaluator\n\nevaluator = MetricEvaluator()\nprint(evaluator.get_available_metrics('classification'))\n</code></pre> <ul> <li><code>accuracy</code> - Overall accuracy</li> <li><code>precision</code> - Positive Predictive Value</li> <li><code>recall</code> - True Positive Rate</li> <li><code>f1</code> - F1 Score</li> <li><code>specificity</code> - True Negative Rate</li> <li><code>npv</code> - Negative Predictive Value</li> <li><code>fpr</code> - False Positive Rate</li> <li><code>auc</code> - ROC AUC Score</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#regression-metrics","title":"Regression Metrics","text":"<pre><code>print(evaluator.get_available_metrics('regression'))\n</code></pre> <ul> <li><code>mae</code> - Mean Absolute Error</li> <li><code>mse</code> - Mean Squared Error</li> <li><code>rmse</code> - Root Mean Squared Error</li> <li><code>r2</code> - R\u00b2 Score</li> <li><code>mape</code> - Mean Absolute Percentage Error</li> <li><code>iou</code> - Intersection over Union</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#detection-metrics","title":"Detection Metrics","text":"<pre><code>print(evaluator.get_available_metrics('detection'))\n</code></pre> <ul> <li><code>map</code> - mean Average Precision @0.5:0.95</li> <li><code>map50</code> - mean Average Precision @0.5</li> <li><code>precision</code> - Detection precision</li> <li><code>recall</code> - Detection recall</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#available-ci-methods","title":"Available CI Methods","text":"","tags":["getting-started"]},{"location":"getting-started/quickstart/#for-classification","title":"For Classification","text":"<pre><code>print(evaluator.get_available_methods('classification'))\n</code></pre> <p>Analytical methods: - <code>wilson</code> - Wilson score (recommended) - <code>normal</code> - Normal approximation - <code>agresti_coull</code> - Agresti-Coull interval - <code>beta</code> - Clopper-Pearson - <code>jeffreys</code> - Jeffreys interval</p> <p>Bootstrap methods: - <code>bootstrap_bca</code> - BCa (recommended) - <code>bootstrap_percentile</code> - Percentile - <code>bootstrap_basic</code> - Basic</p> <p>Other: - <code>jackknife</code> - Jackknife resampling - <code>takahashi</code> - For F1/precision/recall (multi-class)</p>","tags":["getting-started"]},{"location":"getting-started/quickstart/#for-regression","title":"For Regression","text":"<pre><code>print(evaluator.get_available_methods('regression'))\n</code></pre> <ul> <li><code>bootstrap_bca</code> - Bootstrap BCa (default)</li> <li><code>bootstrap_percentile</code> - Bootstrap percentile</li> <li><code>jackknife</code> - Jackknife resampling</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#real-world-example","title":"Real-World Example","text":"","tags":["getting-started"]},{"location":"getting-started/quickstart/#evaluating-a-scikit-learn-model","title":"Evaluating a Scikit-Learn Model","text":"<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom infer_ci import MetricEvaluator\nimport numpy as np\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Binary classification (class 0 vs rest)\ny_binary = (y == 0).astype(int)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_binary, test_size=0.3, random_state=42\n)\n\n# Train model\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Evaluate with confidence intervals\nevaluator = MetricEvaluator()\n\nfor metric in ['accuracy', 'precision', 'recall', 'f1']:\n    value, ci = evaluator.evaluate(\n        y_true=y_test,\n        y_pred=y_pred,\n        task='classification',\n        metric=metric,\n        method='wilson'\n    )\n    print(f\"{metric.capitalize():12s}: {value:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>","tags":["getting-started"]},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Classification Metrics Guide</li> <li>Regression Metrics Guide</li> <li>Object Detection Guide</li> <li>CI Methods Explained</li> <li>API Reference</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#common-parameters","title":"Common Parameters","text":"<p>Most functions accept these parameters:</p> <ul> <li><code>method</code> - CI calculation method (default: 'bootstrap_bca')</li> <li><code>confidence_interval</code> - Confidence level (default: 0.95)</li> <li><code>n_resamples</code> - Bootstrap iterations (default: 2000)</li> <li><code>random_state</code> - Random seed for reproducibility</li> <li><code>compute_ci</code> - Whether to compute CI (default: True)</li> <li><code>plot</code> - Generate visualization (default: False, bootstrap only)</li> </ul>","tags":["getting-started"]},{"location":"getting-started/quickstart/#tips","title":"Tips","text":"<ol> <li>Wilson method is recommended for binary classification metrics</li> <li>Bootstrap BCA is the default and works well for most cases</li> <li>Use <code>random_state</code> for reproducible results</li> <li>Increase <code>n_resamples</code> (e.g., 5000) for more stable CIs</li> <li>Use <code>plot=True</code> with bootstrap methods to visualize the distribution</li> </ol>","tags":["getting-started"]},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Report Issues</li> </ul>","tags":["getting-started"]},{"location":"research/benchmarks/","title":"\ud83d\udcca Benchmarks","text":"<p>This section contains benchmark results of this project.</p>","tags":["rnd","research"]},{"location":"research/references/","title":"\ud83d\udcd1 References","text":"<p>This section contains references to research papers, articles, and other resources related to this project.</p>","tags":["rnd","research"]},{"location":"research/reports/","title":"\ud83d\udcc8 Reports","text":"<p>This section contains result reports related to this project.</p>","tags":["rnd","research"]}]}